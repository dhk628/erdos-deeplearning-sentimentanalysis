{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S1.$ Loading data frames with vectors before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>[0.025059903040528297, -0.0033508273772895336,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>[-0.010023828595876694, -0.022591764107346535,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
       "      <td>[0.021090717986226082, 0.01804506592452526, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>[0.03466595709323883, -0.028959855437278748, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>[-0.019601160660386086, -0.04158126562833786, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8539</th>\n",
       "      <td>1</td>\n",
       "      <td>A real snooze .</td>\n",
       "      <td>[0.028580080717802048, 0.020179858431220055, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8540</th>\n",
       "      <td>2</td>\n",
       "      <td>No surprises .</td>\n",
       "      <td>[-0.04816627874970436, 0.007212366443127394, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8541</th>\n",
       "      <td>4</td>\n",
       "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>[0.023906568065285683, 0.00044874087325297296,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8542</th>\n",
       "      <td>1</td>\n",
       "      <td>Her fans walked out muttering words like `` ho...</td>\n",
       "      <td>[-0.011076121591031551, -0.011684155091643333,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8543</th>\n",
       "      <td>2</td>\n",
       "      <td>In this case zero .</td>\n",
       "      <td>[-0.05022260546684265, -0.0014309081016108394,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8544 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "0         4  The Rock is destined to be the 21st Century 's...   \n",
       "1         5  The gorgeously elaborate continuation of `` Th...   \n",
       "2         4  Singer/composer Bryan Adams contributes a slew...   \n",
       "3         3  You 'd think by now America would have had eno...   \n",
       "4         4               Yet the act is still charming here .   \n",
       "...     ...                                                ...   \n",
       "8539      1                                    A real snooze .   \n",
       "8540      2                                     No surprises .   \n",
       "8541      4  We 've seen the hippie-turned-yuppie plot befo...   \n",
       "8542      1  Her fans walked out muttering words like `` ho...   \n",
       "8543      2                                In this case zero .   \n",
       "\n",
       "                                                 vector  \n",
       "0     [0.025059903040528297, -0.0033508273772895336,...  \n",
       "1     [-0.010023828595876694, -0.022591764107346535,...  \n",
       "2     [0.021090717986226082, 0.01804506592452526, -0...  \n",
       "3     [0.03466595709323883, -0.028959855437278748, -...  \n",
       "4     [-0.019601160660386086, -0.04158126562833786, ...  \n",
       "...                                                 ...  \n",
       "8539  [0.028580080717802048, 0.020179858431220055, 0...  \n",
       "8540  [-0.04816627874970436, 0.007212366443127394, 0...  \n",
       "8541  [0.023906568065285683, 0.00044874087325297296,...  \n",
       "8542  [-0.011076121591031551, -0.011684155091643333,...  \n",
       "8543  [-0.05022260546684265, -0.0014309081016108394,...  \n",
       "\n",
       "[8544 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_parquet(\"SST-5_train_august10.parquet\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>It 's a lovely film with lovely performances b...</td>\n",
       "      <td>[0.030717480927705765, -0.031038176268339157, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>No one goes unindicted here , which is probabl...</td>\n",
       "      <td>[-0.054776858538389206, -0.013850521296262741,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>And if you 're not nearly moved to tears by a ...</td>\n",
       "      <td>[0.057316429913043976, 0.0021277640480548143, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>A warm , funny , engaging film .</td>\n",
       "      <td>[-0.009823216125369072, -0.02523845061659813, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Uses sharp humor and insight into human nature...</td>\n",
       "      <td>[0.002628966700285673, -0.012152031995356083, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>2</td>\n",
       "      <td>it seems to me the film is about the art of ri...</td>\n",
       "      <td>[-0.023168005049228668, 0.0233288686722517, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>2</td>\n",
       "      <td>It 's just disappointingly superficial -- a mo...</td>\n",
       "      <td>[0.003064792137593031, 0.024236295372247696, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>2</td>\n",
       "      <td>The title not only describes its main characte...</td>\n",
       "      <td>[-0.056361518800258636, -0.00899408757686615, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>3</td>\n",
       "      <td>Sometimes it feels as if it might have been ma...</td>\n",
       "      <td>[0.010815807618200779, -0.02518828772008419, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>2</td>\n",
       "      <td>Schaeffer has to find some hook on which to ha...</td>\n",
       "      <td>[0.01869855634868145, -0.017020858824253082, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1101 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "0         4  It 's a lovely film with lovely performances b...   \n",
       "1         3  No one goes unindicted here , which is probabl...   \n",
       "2         4  And if you 're not nearly moved to tears by a ...   \n",
       "3         5                   A warm , funny , engaging film .   \n",
       "4         5  Uses sharp humor and insight into human nature...   \n",
       "...     ...                                                ...   \n",
       "1096      2  it seems to me the film is about the art of ri...   \n",
       "1097      2  It 's just disappointingly superficial -- a mo...   \n",
       "1098      2  The title not only describes its main characte...   \n",
       "1099      3  Sometimes it feels as if it might have been ma...   \n",
       "1100      2  Schaeffer has to find some hook on which to ha...   \n",
       "\n",
       "                                                 vector  \n",
       "0     [0.030717480927705765, -0.031038176268339157, ...  \n",
       "1     [-0.054776858538389206, -0.013850521296262741,...  \n",
       "2     [0.057316429913043976, 0.0021277640480548143, ...  \n",
       "3     [-0.009823216125369072, -0.02523845061659813, ...  \n",
       "4     [0.002628966700285673, -0.012152031995356083, ...  \n",
       "...                                                 ...  \n",
       "1096  [-0.023168005049228668, 0.0233288686722517, -0...  \n",
       "1097  [0.003064792137593031, 0.024236295372247696, 0...  \n",
       "1098  [-0.056361518800258636, -0.00899408757686615, ...  \n",
       "1099  [0.010815807618200779, -0.02518828772008419, 0...  \n",
       "1100  [0.01869855634868145, -0.017020858824253082, -...  \n",
       "\n",
       "[1101 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val = pd.read_parquet(\"SST-5_validation_august10.parquet\")\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>[0.04016323760151863, -0.04099227488040924, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>[0.004230129066854715, 0.02602524869143963, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>[-0.013230101205408573, -0.018727866932749748,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The film provides some great insight into the ...</td>\n",
       "      <td>[0.019872618839144707, -0.0012640012428164482,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>[0.024984797462821007, 0.013797130435705185, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>4</td>\n",
       "      <td>An imaginative comedy/thriller .</td>\n",
       "      <td>[-0.020144404843449593, -0.023559458553791046,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>5</td>\n",
       "      <td>( A ) rare , beautiful film .</td>\n",
       "      <td>[0.029272060841321945, 0.024908604100346565, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>5</td>\n",
       "      <td>( An ) hilarious romantic comedy .</td>\n",
       "      <td>[-0.00450526038184762, -0.04325564205646515, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>4</td>\n",
       "      <td>Never ( sinks ) into exploitation .</td>\n",
       "      <td>[-0.011091343127191067, 0.0322980172932148, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>1</td>\n",
       "      <td>( U ) nrelentingly stupid .</td>\n",
       "      <td>[0.00222303019836545, -0.008687865920364857, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2210 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "0         3                     Effective but too-tepid biopic   \n",
       "1         4  If you sometimes like to go to the movies to h...   \n",
       "2         5  Emerges as something rare , an issue movie tha...   \n",
       "3         3  The film provides some great insight into the ...   \n",
       "4         5  Offers that rare combination of entertainment ...   \n",
       "...     ...                                                ...   \n",
       "2205      4                   An imaginative comedy/thriller .   \n",
       "2206      5                      ( A ) rare , beautiful film .   \n",
       "2207      5                 ( An ) hilarious romantic comedy .   \n",
       "2208      4                Never ( sinks ) into exploitation .   \n",
       "2209      1                        ( U ) nrelentingly stupid .   \n",
       "\n",
       "                                                 vector  \n",
       "0     [0.04016323760151863, -0.04099227488040924, -0...  \n",
       "1     [0.004230129066854715, 0.02602524869143963, -0...  \n",
       "2     [-0.013230101205408573, -0.018727866932749748,...  \n",
       "3     [0.019872618839144707, -0.0012640012428164482,...  \n",
       "4     [0.024984797462821007, 0.013797130435705185, -...  \n",
       "...                                                 ...  \n",
       "2205  [-0.020144404843449593, -0.023559458553791046,...  \n",
       "2206  [0.029272060841321945, 0.024908604100346565, -...  \n",
       "2207  [-0.00450526038184762, -0.04325564205646515, -...  \n",
       "2208  [-0.011091343127191067, 0.0322980172932148, -0...  \n",
       "2209  [0.00222303019836545, -0.008687865920364857, 0...  \n",
       "\n",
       "[2210 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_parquet(\"SST-5_test_august10.parquet\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S2.$ Fine-tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import concatenate_datasets # https://huggingface.co/docs/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts[0] contains label 1\n",
    "# ...\n",
    "# texts[4] contains label 5\n",
    "\n",
    "texts = []\n",
    "\n",
    "for i in range(5):\n",
    "    texts.append( df_train[df_train['label'] == i+1]['text'].tolist() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1092\n",
      "2218\n",
      "1624\n",
      "2322\n",
      "1288\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(len(texts[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. July 30: Telling the model to separate 1-star ratings and 5-star ratings apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating anchor-positive dataset for rating 5\n",
    "\n",
    "len_5 = len(texts[4]) // 2 # 644\n",
    "\n",
    "train_dataset_5_first = Dataset.from_dict({\n",
    "    'anchor': texts[4][0:len_5],\n",
    "    'positive': texts[4][len_5:],\n",
    "    'negative': texts[0][0:len_5]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_5_second = Dataset.from_dict({\n",
    "    'anchor': texts[4][0:len_5],\n",
    "    'positive': texts[4][len_5:],\n",
    "    'negative': texts[0][-len_5:]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['anchor', 'positive', 'negative'],\n",
       "    num_rows: 644\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_5_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating anchor-positive dataset for rating 1\n",
    "\n",
    "len_1 = len(texts[0]) // 2 # 546\n",
    "\n",
    "train_dataset_1_first = Dataset.from_dict({\n",
    "    'anchor': texts[0][0:len_1],\n",
    "    'positive': texts[0][len_1:],\n",
    "    'negative': texts[4][0:len_1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_1_second = Dataset.from_dict({\n",
    "    'anchor': texts[0][0:len_1],\n",
    "    'positive': texts[0][len_1:],\n",
    "    'negative': texts[4][-len_1:]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['anchor', 'positive', 'negative'],\n",
       "    num_rows: 546\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_1_second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. August 10: Telling the model to separate 3-star ratings from extreme ratings: 1-star ratings and 5-star ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating anchor-positive dataset for rating 5\n",
    "\n",
    "len_5 = len(texts[4]) // 2 # 644\n",
    "\n",
    "train_dataset_5_from_3_first = Dataset.from_dict({\n",
    "    'anchor': texts[4][0:len_5],\n",
    "    'positive': texts[4][len_5:],\n",
    "    'negative': texts[2][0:len_5] # index 2 means 3-star ratings\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_5_from_3_second = Dataset.from_dict({\n",
    "    'anchor': texts[4][0:len_5],\n",
    "    'positive': texts[4][len_5:],\n",
    "    'negative': texts[2][-len_5:]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating anchor-positive dataset for rating 1\n",
    "\n",
    "len_1 = len(texts[0]) // 2 # 546\n",
    "\n",
    "train_dataset_1_from_3_first = Dataset.from_dict({\n",
    "    'anchor': texts[0][0:len_1],\n",
    "    'positive': texts[0][len_1:],\n",
    "    'negative': texts[2][0:len_1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_1_from_3_second = Dataset.from_dict({\n",
    "    'anchor': texts[0][0:len_1],\n",
    "    'positive': texts[0][len_1:],\n",
    "    'negative': texts[2][-len_1:]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate datasets -- https://huggingface.co/docs/datasets/v1.3.0/processing.html\n",
    "\n",
    "train_dataset = concatenate_datasets(\n",
    "    [\n",
    "        train_dataset_5_from_3_first,\n",
    "        train_dataset_5_from_3_second,\n",
    "        train_dataset_1_from_3_first,\n",
    "        train_dataset_1_from_3_second\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. August 11: Telling the model to separate 3-star ratings from extreme ratings: 2-star ratings and 4-star ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating anchor-positive dataset for rating 4\n",
    "\n",
    "len_4 = len(texts[3]) // 2 # index 4 means 4-star ratings\n",
    "\n",
    "train_dataset_4_from_3_first = Dataset.from_dict({\n",
    "    'anchor': texts[3][0:len_4],\n",
    "    'positive': texts[3][len_4:],\n",
    "    'negative': texts[2][0:len_4] # index 2 means 3-star ratings\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_4_from_3_second = Dataset.from_dict({\n",
    "    'anchor': texts[3][0:len_4],\n",
    "    'positive': texts[3][len_4:],\n",
    "    'negative': texts[2][-len_4:]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating anchor-positive dataset for rating 2\n",
    "\n",
    "len_2 = len(texts[1]) // 2 #  index 1 means 2-star ratings\n",
    "\n",
    "train_dataset_2_from_3_first = Dataset.from_dict({\n",
    "    'anchor': texts[1][:len_2],\n",
    "    'positive': texts[1][len_2:],\n",
    "    'negative': texts[2][:len_2] # index 2 means 3-star ratings\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_2_from_3_second = Dataset.from_dict({\n",
    "    'anchor': texts[1][:len_2],\n",
    "    'positive': texts[1][len_2:],\n",
    "    'negative': texts[2][-len_2:]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate datasets -- https://huggingface.co/docs/datasets/v1.3.0/processing.html\n",
    "\n",
    "train_dataset = concatenate_datasets(\n",
    "    [\n",
    "        train_dataset_4_from_3_first,\n",
    "        train_dataset_4_from_3_second,\n",
    "        train_dataset_2_from_3_first,\n",
    "        train_dataset_2_from_3_second\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S3.$ Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments, losses\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "\n",
    "# https://www.sbert.net/docs/sentence_transformer/training_overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('fine_tuned_gte_august10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._first_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 1024)\n",
       "    (token_type_embeddings): Embedding(2, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_model = model._first_module().auto_model\n",
    "auto_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertPooler(\n",
       "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_model.pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 1024)\n",
       "    (token_type_embeddings): Embedding(2, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "encoder.layer.12.attention.self.query.weight\n",
      "encoder.layer.12.attention.self.query.bias\n",
      "encoder.layer.12.attention.self.key.weight\n",
      "encoder.layer.12.attention.self.key.bias\n",
      "encoder.layer.12.attention.self.value.weight\n",
      "encoder.layer.12.attention.self.value.bias\n",
      "encoder.layer.12.attention.output.dense.weight\n",
      "encoder.layer.12.attention.output.dense.bias\n",
      "encoder.layer.12.attention.output.LayerNorm.weight\n",
      "encoder.layer.12.attention.output.LayerNorm.bias\n",
      "encoder.layer.12.intermediate.dense.weight\n",
      "encoder.layer.12.intermediate.dense.bias\n",
      "encoder.layer.12.output.dense.weight\n",
      "encoder.layer.12.output.dense.bias\n",
      "encoder.layer.12.output.LayerNorm.weight\n",
      "encoder.layer.12.output.LayerNorm.bias\n",
      "encoder.layer.13.attention.self.query.weight\n",
      "encoder.layer.13.attention.self.query.bias\n",
      "encoder.layer.13.attention.self.key.weight\n",
      "encoder.layer.13.attention.self.key.bias\n",
      "encoder.layer.13.attention.self.value.weight\n",
      "encoder.layer.13.attention.self.value.bias\n",
      "encoder.layer.13.attention.output.dense.weight\n",
      "encoder.layer.13.attention.output.dense.bias\n",
      "encoder.layer.13.attention.output.LayerNorm.weight\n",
      "encoder.layer.13.attention.output.LayerNorm.bias\n",
      "encoder.layer.13.intermediate.dense.weight\n",
      "encoder.layer.13.intermediate.dense.bias\n",
      "encoder.layer.13.output.dense.weight\n",
      "encoder.layer.13.output.dense.bias\n",
      "encoder.layer.13.output.LayerNorm.weight\n",
      "encoder.layer.13.output.LayerNorm.bias\n",
      "encoder.layer.14.attention.self.query.weight\n",
      "encoder.layer.14.attention.self.query.bias\n",
      "encoder.layer.14.attention.self.key.weight\n",
      "encoder.layer.14.attention.self.key.bias\n",
      "encoder.layer.14.attention.self.value.weight\n",
      "encoder.layer.14.attention.self.value.bias\n",
      "encoder.layer.14.attention.output.dense.weight\n",
      "encoder.layer.14.attention.output.dense.bias\n",
      "encoder.layer.14.attention.output.LayerNorm.weight\n",
      "encoder.layer.14.attention.output.LayerNorm.bias\n",
      "encoder.layer.14.intermediate.dense.weight\n",
      "encoder.layer.14.intermediate.dense.bias\n",
      "encoder.layer.14.output.dense.weight\n",
      "encoder.layer.14.output.dense.bias\n",
      "encoder.layer.14.output.LayerNorm.weight\n",
      "encoder.layer.14.output.LayerNorm.bias\n",
      "encoder.layer.15.attention.self.query.weight\n",
      "encoder.layer.15.attention.self.query.bias\n",
      "encoder.layer.15.attention.self.key.weight\n",
      "encoder.layer.15.attention.self.key.bias\n",
      "encoder.layer.15.attention.self.value.weight\n",
      "encoder.layer.15.attention.self.value.bias\n",
      "encoder.layer.15.attention.output.dense.weight\n",
      "encoder.layer.15.attention.output.dense.bias\n",
      "encoder.layer.15.attention.output.LayerNorm.weight\n",
      "encoder.layer.15.attention.output.LayerNorm.bias\n",
      "encoder.layer.15.intermediate.dense.weight\n",
      "encoder.layer.15.intermediate.dense.bias\n",
      "encoder.layer.15.output.dense.weight\n",
      "encoder.layer.15.output.dense.bias\n",
      "encoder.layer.15.output.LayerNorm.weight\n",
      "encoder.layer.15.output.LayerNorm.bias\n",
      "encoder.layer.16.attention.self.query.weight\n",
      "encoder.layer.16.attention.self.query.bias\n",
      "encoder.layer.16.attention.self.key.weight\n",
      "encoder.layer.16.attention.self.key.bias\n",
      "encoder.layer.16.attention.self.value.weight\n",
      "encoder.layer.16.attention.self.value.bias\n",
      "encoder.layer.16.attention.output.dense.weight\n",
      "encoder.layer.16.attention.output.dense.bias\n",
      "encoder.layer.16.attention.output.LayerNorm.weight\n",
      "encoder.layer.16.attention.output.LayerNorm.bias\n",
      "encoder.layer.16.intermediate.dense.weight\n",
      "encoder.layer.16.intermediate.dense.bias\n",
      "encoder.layer.16.output.dense.weight\n",
      "encoder.layer.16.output.dense.bias\n",
      "encoder.layer.16.output.LayerNorm.weight\n",
      "encoder.layer.16.output.LayerNorm.bias\n",
      "encoder.layer.17.attention.self.query.weight\n",
      "encoder.layer.17.attention.self.query.bias\n",
      "encoder.layer.17.attention.self.key.weight\n",
      "encoder.layer.17.attention.self.key.bias\n",
      "encoder.layer.17.attention.self.value.weight\n",
      "encoder.layer.17.attention.self.value.bias\n",
      "encoder.layer.17.attention.output.dense.weight\n",
      "encoder.layer.17.attention.output.dense.bias\n",
      "encoder.layer.17.attention.output.LayerNorm.weight\n",
      "encoder.layer.17.attention.output.LayerNorm.bias\n",
      "encoder.layer.17.intermediate.dense.weight\n",
      "encoder.layer.17.intermediate.dense.bias\n",
      "encoder.layer.17.output.dense.weight\n",
      "encoder.layer.17.output.dense.bias\n",
      "encoder.layer.17.output.LayerNorm.weight\n",
      "encoder.layer.17.output.LayerNorm.bias\n",
      "encoder.layer.18.attention.self.query.weight\n",
      "encoder.layer.18.attention.self.query.bias\n",
      "encoder.layer.18.attention.self.key.weight\n",
      "encoder.layer.18.attention.self.key.bias\n",
      "encoder.layer.18.attention.self.value.weight\n",
      "encoder.layer.18.attention.self.value.bias\n",
      "encoder.layer.18.attention.output.dense.weight\n",
      "encoder.layer.18.attention.output.dense.bias\n",
      "encoder.layer.18.attention.output.LayerNorm.weight\n",
      "encoder.layer.18.attention.output.LayerNorm.bias\n",
      "encoder.layer.18.intermediate.dense.weight\n",
      "encoder.layer.18.intermediate.dense.bias\n",
      "encoder.layer.18.output.dense.weight\n",
      "encoder.layer.18.output.dense.bias\n",
      "encoder.layer.18.output.LayerNorm.weight\n",
      "encoder.layer.18.output.LayerNorm.bias\n",
      "encoder.layer.19.attention.self.query.weight\n",
      "encoder.layer.19.attention.self.query.bias\n",
      "encoder.layer.19.attention.self.key.weight\n",
      "encoder.layer.19.attention.self.key.bias\n",
      "encoder.layer.19.attention.self.value.weight\n",
      "encoder.layer.19.attention.self.value.bias\n",
      "encoder.layer.19.attention.output.dense.weight\n",
      "encoder.layer.19.attention.output.dense.bias\n",
      "encoder.layer.19.attention.output.LayerNorm.weight\n",
      "encoder.layer.19.attention.output.LayerNorm.bias\n",
      "encoder.layer.19.intermediate.dense.weight\n",
      "encoder.layer.19.intermediate.dense.bias\n",
      "encoder.layer.19.output.dense.weight\n",
      "encoder.layer.19.output.dense.bias\n",
      "encoder.layer.19.output.LayerNorm.weight\n",
      "encoder.layer.19.output.LayerNorm.bias\n",
      "encoder.layer.20.attention.self.query.weight\n",
      "encoder.layer.20.attention.self.query.bias\n",
      "encoder.layer.20.attention.self.key.weight\n",
      "encoder.layer.20.attention.self.key.bias\n",
      "encoder.layer.20.attention.self.value.weight\n",
      "encoder.layer.20.attention.self.value.bias\n",
      "encoder.layer.20.attention.output.dense.weight\n",
      "encoder.layer.20.attention.output.dense.bias\n",
      "encoder.layer.20.attention.output.LayerNorm.weight\n",
      "encoder.layer.20.attention.output.LayerNorm.bias\n",
      "encoder.layer.20.intermediate.dense.weight\n",
      "encoder.layer.20.intermediate.dense.bias\n",
      "encoder.layer.20.output.dense.weight\n",
      "encoder.layer.20.output.dense.bias\n",
      "encoder.layer.20.output.LayerNorm.weight\n",
      "encoder.layer.20.output.LayerNorm.bias\n",
      "encoder.layer.21.attention.self.query.weight\n",
      "encoder.layer.21.attention.self.query.bias\n",
      "encoder.layer.21.attention.self.key.weight\n",
      "encoder.layer.21.attention.self.key.bias\n",
      "encoder.layer.21.attention.self.value.weight\n",
      "encoder.layer.21.attention.self.value.bias\n",
      "encoder.layer.21.attention.output.dense.weight\n",
      "encoder.layer.21.attention.output.dense.bias\n",
      "encoder.layer.21.attention.output.LayerNorm.weight\n",
      "encoder.layer.21.attention.output.LayerNorm.bias\n",
      "encoder.layer.21.intermediate.dense.weight\n",
      "encoder.layer.21.intermediate.dense.bias\n",
      "encoder.layer.21.output.dense.weight\n",
      "encoder.layer.21.output.dense.bias\n",
      "encoder.layer.21.output.LayerNorm.weight\n",
      "encoder.layer.21.output.LayerNorm.bias\n",
      "encoder.layer.22.attention.self.query.weight\n",
      "encoder.layer.22.attention.self.query.bias\n",
      "encoder.layer.22.attention.self.key.weight\n",
      "encoder.layer.22.attention.self.key.bias\n",
      "encoder.layer.22.attention.self.value.weight\n",
      "encoder.layer.22.attention.self.value.bias\n",
      "encoder.layer.22.attention.output.dense.weight\n",
      "encoder.layer.22.attention.output.dense.bias\n",
      "encoder.layer.22.attention.output.LayerNorm.weight\n",
      "encoder.layer.22.attention.output.LayerNorm.bias\n",
      "encoder.layer.22.intermediate.dense.weight\n",
      "encoder.layer.22.intermediate.dense.bias\n",
      "encoder.layer.22.output.dense.weight\n",
      "encoder.layer.22.output.dense.bias\n",
      "encoder.layer.22.output.LayerNorm.weight\n",
      "encoder.layer.22.output.LayerNorm.bias\n",
      "encoder.layer.23.attention.self.query.weight\n",
      "encoder.layer.23.attention.self.query.bias\n",
      "encoder.layer.23.attention.self.key.weight\n",
      "encoder.layer.23.attention.self.key.bias\n",
      "encoder.layer.23.attention.self.value.weight\n",
      "encoder.layer.23.attention.self.value.bias\n",
      "encoder.layer.23.attention.output.dense.weight\n",
      "encoder.layer.23.attention.output.dense.bias\n",
      "encoder.layer.23.attention.output.LayerNorm.weight\n",
      "encoder.layer.23.attention.output.LayerNorm.bias\n",
      "encoder.layer.23.intermediate.dense.weight\n",
      "encoder.layer.23.intermediate.dense.bias\n",
      "encoder.layer.23.output.dense.weight\n",
      "encoder.layer.23.output.dense.bias\n",
      "encoder.layer.23.output.LayerNorm.weight\n",
      "encoder.layer.23.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in auto_model.named_parameters():\n",
    "    print(name)\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in auto_model.named_parameters():\n",
    "    for i in range(20, 24):\n",
    "        if name == f'encoder.layer.{i}.output.dense.weight':\n",
    "            param.requires_grad = True\n",
    "        elif name == f'encoder.layer.{i}.output.dense.bias':\n",
    "            param.requires_grad = True\n",
    "        elif name == f'encoder.layer.{i}.intermediate.dense.weight':\n",
    "            param.requires_grad = True\n",
    "        elif name == f'encoder.layer.{i}.intermediate.dense.bias':\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight :  False\n",
      "embeddings.position_embeddings.weight :  False\n",
      "embeddings.token_type_embeddings.weight :  False\n",
      "embeddings.LayerNorm.weight :  False\n",
      "embeddings.LayerNorm.bias :  False\n",
      "encoder.layer.0.attention.self.query.weight :  False\n",
      "encoder.layer.0.attention.self.query.bias :  False\n",
      "encoder.layer.0.attention.self.key.weight :  False\n",
      "encoder.layer.0.attention.self.key.bias :  False\n",
      "encoder.layer.0.attention.self.value.weight :  False\n",
      "encoder.layer.0.attention.self.value.bias :  False\n",
      "encoder.layer.0.attention.output.dense.weight :  False\n",
      "encoder.layer.0.attention.output.dense.bias :  False\n",
      "encoder.layer.0.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.0.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.0.intermediate.dense.weight :  False\n",
      "encoder.layer.0.intermediate.dense.bias :  False\n",
      "encoder.layer.0.output.dense.weight :  False\n",
      "encoder.layer.0.output.dense.bias :  False\n",
      "encoder.layer.0.output.LayerNorm.weight :  False\n",
      "encoder.layer.0.output.LayerNorm.bias :  False\n",
      "encoder.layer.1.attention.self.query.weight :  False\n",
      "encoder.layer.1.attention.self.query.bias :  False\n",
      "encoder.layer.1.attention.self.key.weight :  False\n",
      "encoder.layer.1.attention.self.key.bias :  False\n",
      "encoder.layer.1.attention.self.value.weight :  False\n",
      "encoder.layer.1.attention.self.value.bias :  False\n",
      "encoder.layer.1.attention.output.dense.weight :  False\n",
      "encoder.layer.1.attention.output.dense.bias :  False\n",
      "encoder.layer.1.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.1.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.1.intermediate.dense.weight :  False\n",
      "encoder.layer.1.intermediate.dense.bias :  False\n",
      "encoder.layer.1.output.dense.weight :  False\n",
      "encoder.layer.1.output.dense.bias :  False\n",
      "encoder.layer.1.output.LayerNorm.weight :  False\n",
      "encoder.layer.1.output.LayerNorm.bias :  False\n",
      "encoder.layer.2.attention.self.query.weight :  False\n",
      "encoder.layer.2.attention.self.query.bias :  False\n",
      "encoder.layer.2.attention.self.key.weight :  False\n",
      "encoder.layer.2.attention.self.key.bias :  False\n",
      "encoder.layer.2.attention.self.value.weight :  False\n",
      "encoder.layer.2.attention.self.value.bias :  False\n",
      "encoder.layer.2.attention.output.dense.weight :  False\n",
      "encoder.layer.2.attention.output.dense.bias :  False\n",
      "encoder.layer.2.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.2.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.2.intermediate.dense.weight :  False\n",
      "encoder.layer.2.intermediate.dense.bias :  False\n",
      "encoder.layer.2.output.dense.weight :  False\n",
      "encoder.layer.2.output.dense.bias :  False\n",
      "encoder.layer.2.output.LayerNorm.weight :  False\n",
      "encoder.layer.2.output.LayerNorm.bias :  False\n",
      "encoder.layer.3.attention.self.query.weight :  False\n",
      "encoder.layer.3.attention.self.query.bias :  False\n",
      "encoder.layer.3.attention.self.key.weight :  False\n",
      "encoder.layer.3.attention.self.key.bias :  False\n",
      "encoder.layer.3.attention.self.value.weight :  False\n",
      "encoder.layer.3.attention.self.value.bias :  False\n",
      "encoder.layer.3.attention.output.dense.weight :  False\n",
      "encoder.layer.3.attention.output.dense.bias :  False\n",
      "encoder.layer.3.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.3.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.3.intermediate.dense.weight :  False\n",
      "encoder.layer.3.intermediate.dense.bias :  False\n",
      "encoder.layer.3.output.dense.weight :  False\n",
      "encoder.layer.3.output.dense.bias :  False\n",
      "encoder.layer.3.output.LayerNorm.weight :  False\n",
      "encoder.layer.3.output.LayerNorm.bias :  False\n",
      "encoder.layer.4.attention.self.query.weight :  False\n",
      "encoder.layer.4.attention.self.query.bias :  False\n",
      "encoder.layer.4.attention.self.key.weight :  False\n",
      "encoder.layer.4.attention.self.key.bias :  False\n",
      "encoder.layer.4.attention.self.value.weight :  False\n",
      "encoder.layer.4.attention.self.value.bias :  False\n",
      "encoder.layer.4.attention.output.dense.weight :  False\n",
      "encoder.layer.4.attention.output.dense.bias :  False\n",
      "encoder.layer.4.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.4.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.4.intermediate.dense.weight :  False\n",
      "encoder.layer.4.intermediate.dense.bias :  False\n",
      "encoder.layer.4.output.dense.weight :  False\n",
      "encoder.layer.4.output.dense.bias :  False\n",
      "encoder.layer.4.output.LayerNorm.weight :  False\n",
      "encoder.layer.4.output.LayerNorm.bias :  False\n",
      "encoder.layer.5.attention.self.query.weight :  False\n",
      "encoder.layer.5.attention.self.query.bias :  False\n",
      "encoder.layer.5.attention.self.key.weight :  False\n",
      "encoder.layer.5.attention.self.key.bias :  False\n",
      "encoder.layer.5.attention.self.value.weight :  False\n",
      "encoder.layer.5.attention.self.value.bias :  False\n",
      "encoder.layer.5.attention.output.dense.weight :  False\n",
      "encoder.layer.5.attention.output.dense.bias :  False\n",
      "encoder.layer.5.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.5.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.5.intermediate.dense.weight :  False\n",
      "encoder.layer.5.intermediate.dense.bias :  False\n",
      "encoder.layer.5.output.dense.weight :  False\n",
      "encoder.layer.5.output.dense.bias :  False\n",
      "encoder.layer.5.output.LayerNorm.weight :  False\n",
      "encoder.layer.5.output.LayerNorm.bias :  False\n",
      "encoder.layer.6.attention.self.query.weight :  False\n",
      "encoder.layer.6.attention.self.query.bias :  False\n",
      "encoder.layer.6.attention.self.key.weight :  False\n",
      "encoder.layer.6.attention.self.key.bias :  False\n",
      "encoder.layer.6.attention.self.value.weight :  False\n",
      "encoder.layer.6.attention.self.value.bias :  False\n",
      "encoder.layer.6.attention.output.dense.weight :  False\n",
      "encoder.layer.6.attention.output.dense.bias :  False\n",
      "encoder.layer.6.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.6.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.6.intermediate.dense.weight :  False\n",
      "encoder.layer.6.intermediate.dense.bias :  False\n",
      "encoder.layer.6.output.dense.weight :  False\n",
      "encoder.layer.6.output.dense.bias :  False\n",
      "encoder.layer.6.output.LayerNorm.weight :  False\n",
      "encoder.layer.6.output.LayerNorm.bias :  False\n",
      "encoder.layer.7.attention.self.query.weight :  False\n",
      "encoder.layer.7.attention.self.query.bias :  False\n",
      "encoder.layer.7.attention.self.key.weight :  False\n",
      "encoder.layer.7.attention.self.key.bias :  False\n",
      "encoder.layer.7.attention.self.value.weight :  False\n",
      "encoder.layer.7.attention.self.value.bias :  False\n",
      "encoder.layer.7.attention.output.dense.weight :  False\n",
      "encoder.layer.7.attention.output.dense.bias :  False\n",
      "encoder.layer.7.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.7.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.7.intermediate.dense.weight :  False\n",
      "encoder.layer.7.intermediate.dense.bias :  False\n",
      "encoder.layer.7.output.dense.weight :  False\n",
      "encoder.layer.7.output.dense.bias :  False\n",
      "encoder.layer.7.output.LayerNorm.weight :  False\n",
      "encoder.layer.7.output.LayerNorm.bias :  False\n",
      "encoder.layer.8.attention.self.query.weight :  False\n",
      "encoder.layer.8.attention.self.query.bias :  False\n",
      "encoder.layer.8.attention.self.key.weight :  False\n",
      "encoder.layer.8.attention.self.key.bias :  False\n",
      "encoder.layer.8.attention.self.value.weight :  False\n",
      "encoder.layer.8.attention.self.value.bias :  False\n",
      "encoder.layer.8.attention.output.dense.weight :  False\n",
      "encoder.layer.8.attention.output.dense.bias :  False\n",
      "encoder.layer.8.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.8.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.8.intermediate.dense.weight :  False\n",
      "encoder.layer.8.intermediate.dense.bias :  False\n",
      "encoder.layer.8.output.dense.weight :  False\n",
      "encoder.layer.8.output.dense.bias :  False\n",
      "encoder.layer.8.output.LayerNorm.weight :  False\n",
      "encoder.layer.8.output.LayerNorm.bias :  False\n",
      "encoder.layer.9.attention.self.query.weight :  False\n",
      "encoder.layer.9.attention.self.query.bias :  False\n",
      "encoder.layer.9.attention.self.key.weight :  False\n",
      "encoder.layer.9.attention.self.key.bias :  False\n",
      "encoder.layer.9.attention.self.value.weight :  False\n",
      "encoder.layer.9.attention.self.value.bias :  False\n",
      "encoder.layer.9.attention.output.dense.weight :  False\n",
      "encoder.layer.9.attention.output.dense.bias :  False\n",
      "encoder.layer.9.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.9.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.9.intermediate.dense.weight :  False\n",
      "encoder.layer.9.intermediate.dense.bias :  False\n",
      "encoder.layer.9.output.dense.weight :  False\n",
      "encoder.layer.9.output.dense.bias :  False\n",
      "encoder.layer.9.output.LayerNorm.weight :  False\n",
      "encoder.layer.9.output.LayerNorm.bias :  False\n",
      "encoder.layer.10.attention.self.query.weight :  False\n",
      "encoder.layer.10.attention.self.query.bias :  False\n",
      "encoder.layer.10.attention.self.key.weight :  False\n",
      "encoder.layer.10.attention.self.key.bias :  False\n",
      "encoder.layer.10.attention.self.value.weight :  False\n",
      "encoder.layer.10.attention.self.value.bias :  False\n",
      "encoder.layer.10.attention.output.dense.weight :  False\n",
      "encoder.layer.10.attention.output.dense.bias :  False\n",
      "encoder.layer.10.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.10.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.10.intermediate.dense.weight :  False\n",
      "encoder.layer.10.intermediate.dense.bias :  False\n",
      "encoder.layer.10.output.dense.weight :  False\n",
      "encoder.layer.10.output.dense.bias :  False\n",
      "encoder.layer.10.output.LayerNorm.weight :  False\n",
      "encoder.layer.10.output.LayerNorm.bias :  False\n",
      "encoder.layer.11.attention.self.query.weight :  False\n",
      "encoder.layer.11.attention.self.query.bias :  False\n",
      "encoder.layer.11.attention.self.key.weight :  False\n",
      "encoder.layer.11.attention.self.key.bias :  False\n",
      "encoder.layer.11.attention.self.value.weight :  False\n",
      "encoder.layer.11.attention.self.value.bias :  False\n",
      "encoder.layer.11.attention.output.dense.weight :  False\n",
      "encoder.layer.11.attention.output.dense.bias :  False\n",
      "encoder.layer.11.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.11.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.11.intermediate.dense.weight :  False\n",
      "encoder.layer.11.intermediate.dense.bias :  False\n",
      "encoder.layer.11.output.dense.weight :  False\n",
      "encoder.layer.11.output.dense.bias :  False\n",
      "encoder.layer.11.output.LayerNorm.weight :  False\n",
      "encoder.layer.11.output.LayerNorm.bias :  False\n",
      "encoder.layer.12.attention.self.query.weight :  False\n",
      "encoder.layer.12.attention.self.query.bias :  False\n",
      "encoder.layer.12.attention.self.key.weight :  False\n",
      "encoder.layer.12.attention.self.key.bias :  False\n",
      "encoder.layer.12.attention.self.value.weight :  False\n",
      "encoder.layer.12.attention.self.value.bias :  False\n",
      "encoder.layer.12.attention.output.dense.weight :  False\n",
      "encoder.layer.12.attention.output.dense.bias :  False\n",
      "encoder.layer.12.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.12.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.12.intermediate.dense.weight :  False\n",
      "encoder.layer.12.intermediate.dense.bias :  False\n",
      "encoder.layer.12.output.dense.weight :  False\n",
      "encoder.layer.12.output.dense.bias :  False\n",
      "encoder.layer.12.output.LayerNorm.weight :  False\n",
      "encoder.layer.12.output.LayerNorm.bias :  False\n",
      "encoder.layer.13.attention.self.query.weight :  False\n",
      "encoder.layer.13.attention.self.query.bias :  False\n",
      "encoder.layer.13.attention.self.key.weight :  False\n",
      "encoder.layer.13.attention.self.key.bias :  False\n",
      "encoder.layer.13.attention.self.value.weight :  False\n",
      "encoder.layer.13.attention.self.value.bias :  False\n",
      "encoder.layer.13.attention.output.dense.weight :  False\n",
      "encoder.layer.13.attention.output.dense.bias :  False\n",
      "encoder.layer.13.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.13.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.13.intermediate.dense.weight :  False\n",
      "encoder.layer.13.intermediate.dense.bias :  False\n",
      "encoder.layer.13.output.dense.weight :  False\n",
      "encoder.layer.13.output.dense.bias :  False\n",
      "encoder.layer.13.output.LayerNorm.weight :  False\n",
      "encoder.layer.13.output.LayerNorm.bias :  False\n",
      "encoder.layer.14.attention.self.query.weight :  False\n",
      "encoder.layer.14.attention.self.query.bias :  False\n",
      "encoder.layer.14.attention.self.key.weight :  False\n",
      "encoder.layer.14.attention.self.key.bias :  False\n",
      "encoder.layer.14.attention.self.value.weight :  False\n",
      "encoder.layer.14.attention.self.value.bias :  False\n",
      "encoder.layer.14.attention.output.dense.weight :  False\n",
      "encoder.layer.14.attention.output.dense.bias :  False\n",
      "encoder.layer.14.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.14.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.14.intermediate.dense.weight :  False\n",
      "encoder.layer.14.intermediate.dense.bias :  False\n",
      "encoder.layer.14.output.dense.weight :  False\n",
      "encoder.layer.14.output.dense.bias :  False\n",
      "encoder.layer.14.output.LayerNorm.weight :  False\n",
      "encoder.layer.14.output.LayerNorm.bias :  False\n",
      "encoder.layer.15.attention.self.query.weight :  False\n",
      "encoder.layer.15.attention.self.query.bias :  False\n",
      "encoder.layer.15.attention.self.key.weight :  False\n",
      "encoder.layer.15.attention.self.key.bias :  False\n",
      "encoder.layer.15.attention.self.value.weight :  False\n",
      "encoder.layer.15.attention.self.value.bias :  False\n",
      "encoder.layer.15.attention.output.dense.weight :  False\n",
      "encoder.layer.15.attention.output.dense.bias :  False\n",
      "encoder.layer.15.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.15.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.15.intermediate.dense.weight :  False\n",
      "encoder.layer.15.intermediate.dense.bias :  False\n",
      "encoder.layer.15.output.dense.weight :  False\n",
      "encoder.layer.15.output.dense.bias :  False\n",
      "encoder.layer.15.output.LayerNorm.weight :  False\n",
      "encoder.layer.15.output.LayerNorm.bias :  False\n",
      "encoder.layer.16.attention.self.query.weight :  False\n",
      "encoder.layer.16.attention.self.query.bias :  False\n",
      "encoder.layer.16.attention.self.key.weight :  False\n",
      "encoder.layer.16.attention.self.key.bias :  False\n",
      "encoder.layer.16.attention.self.value.weight :  False\n",
      "encoder.layer.16.attention.self.value.bias :  False\n",
      "encoder.layer.16.attention.output.dense.weight :  False\n",
      "encoder.layer.16.attention.output.dense.bias :  False\n",
      "encoder.layer.16.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.16.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.16.intermediate.dense.weight :  False\n",
      "encoder.layer.16.intermediate.dense.bias :  False\n",
      "encoder.layer.16.output.dense.weight :  False\n",
      "encoder.layer.16.output.dense.bias :  False\n",
      "encoder.layer.16.output.LayerNorm.weight :  False\n",
      "encoder.layer.16.output.LayerNorm.bias :  False\n",
      "encoder.layer.17.attention.self.query.weight :  False\n",
      "encoder.layer.17.attention.self.query.bias :  False\n",
      "encoder.layer.17.attention.self.key.weight :  False\n",
      "encoder.layer.17.attention.self.key.bias :  False\n",
      "encoder.layer.17.attention.self.value.weight :  False\n",
      "encoder.layer.17.attention.self.value.bias :  False\n",
      "encoder.layer.17.attention.output.dense.weight :  False\n",
      "encoder.layer.17.attention.output.dense.bias :  False\n",
      "encoder.layer.17.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.17.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.17.intermediate.dense.weight :  False\n",
      "encoder.layer.17.intermediate.dense.bias :  False\n",
      "encoder.layer.17.output.dense.weight :  False\n",
      "encoder.layer.17.output.dense.bias :  False\n",
      "encoder.layer.17.output.LayerNorm.weight :  False\n",
      "encoder.layer.17.output.LayerNorm.bias :  False\n",
      "encoder.layer.18.attention.self.query.weight :  False\n",
      "encoder.layer.18.attention.self.query.bias :  False\n",
      "encoder.layer.18.attention.self.key.weight :  False\n",
      "encoder.layer.18.attention.self.key.bias :  False\n",
      "encoder.layer.18.attention.self.value.weight :  False\n",
      "encoder.layer.18.attention.self.value.bias :  False\n",
      "encoder.layer.18.attention.output.dense.weight :  False\n",
      "encoder.layer.18.attention.output.dense.bias :  False\n",
      "encoder.layer.18.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.18.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.18.intermediate.dense.weight :  False\n",
      "encoder.layer.18.intermediate.dense.bias :  False\n",
      "encoder.layer.18.output.dense.weight :  False\n",
      "encoder.layer.18.output.dense.bias :  False\n",
      "encoder.layer.18.output.LayerNorm.weight :  False\n",
      "encoder.layer.18.output.LayerNorm.bias :  False\n",
      "encoder.layer.19.attention.self.query.weight :  False\n",
      "encoder.layer.19.attention.self.query.bias :  False\n",
      "encoder.layer.19.attention.self.key.weight :  False\n",
      "encoder.layer.19.attention.self.key.bias :  False\n",
      "encoder.layer.19.attention.self.value.weight :  False\n",
      "encoder.layer.19.attention.self.value.bias :  False\n",
      "encoder.layer.19.attention.output.dense.weight :  False\n",
      "encoder.layer.19.attention.output.dense.bias :  False\n",
      "encoder.layer.19.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.19.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.19.intermediate.dense.weight :  False\n",
      "encoder.layer.19.intermediate.dense.bias :  False\n",
      "encoder.layer.19.output.dense.weight :  False\n",
      "encoder.layer.19.output.dense.bias :  False\n",
      "encoder.layer.19.output.LayerNorm.weight :  False\n",
      "encoder.layer.19.output.LayerNorm.bias :  False\n",
      "encoder.layer.20.attention.self.query.weight :  False\n",
      "encoder.layer.20.attention.self.query.bias :  False\n",
      "encoder.layer.20.attention.self.key.weight :  False\n",
      "encoder.layer.20.attention.self.key.bias :  False\n",
      "encoder.layer.20.attention.self.value.weight :  False\n",
      "encoder.layer.20.attention.self.value.bias :  False\n",
      "encoder.layer.20.attention.output.dense.weight :  False\n",
      "encoder.layer.20.attention.output.dense.bias :  False\n",
      "encoder.layer.20.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.20.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.20.intermediate.dense.weight :  True\n",
      "encoder.layer.20.intermediate.dense.bias :  True\n",
      "encoder.layer.20.output.dense.weight :  True\n",
      "encoder.layer.20.output.dense.bias :  True\n",
      "encoder.layer.20.output.LayerNorm.weight :  False\n",
      "encoder.layer.20.output.LayerNorm.bias :  False\n",
      "encoder.layer.21.attention.self.query.weight :  False\n",
      "encoder.layer.21.attention.self.query.bias :  False\n",
      "encoder.layer.21.attention.self.key.weight :  False\n",
      "encoder.layer.21.attention.self.key.bias :  False\n",
      "encoder.layer.21.attention.self.value.weight :  False\n",
      "encoder.layer.21.attention.self.value.bias :  False\n",
      "encoder.layer.21.attention.output.dense.weight :  False\n",
      "encoder.layer.21.attention.output.dense.bias :  False\n",
      "encoder.layer.21.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.21.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.21.intermediate.dense.weight :  True\n",
      "encoder.layer.21.intermediate.dense.bias :  True\n",
      "encoder.layer.21.output.dense.weight :  True\n",
      "encoder.layer.21.output.dense.bias :  True\n",
      "encoder.layer.21.output.LayerNorm.weight :  False\n",
      "encoder.layer.21.output.LayerNorm.bias :  False\n",
      "encoder.layer.22.attention.self.query.weight :  False\n",
      "encoder.layer.22.attention.self.query.bias :  False\n",
      "encoder.layer.22.attention.self.key.weight :  False\n",
      "encoder.layer.22.attention.self.key.bias :  False\n",
      "encoder.layer.22.attention.self.value.weight :  False\n",
      "encoder.layer.22.attention.self.value.bias :  False\n",
      "encoder.layer.22.attention.output.dense.weight :  False\n",
      "encoder.layer.22.attention.output.dense.bias :  False\n",
      "encoder.layer.22.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.22.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.22.intermediate.dense.weight :  True\n",
      "encoder.layer.22.intermediate.dense.bias :  True\n",
      "encoder.layer.22.output.dense.weight :  True\n",
      "encoder.layer.22.output.dense.bias :  True\n",
      "encoder.layer.22.output.LayerNorm.weight :  False\n",
      "encoder.layer.22.output.LayerNorm.bias :  False\n",
      "encoder.layer.23.attention.self.query.weight :  False\n",
      "encoder.layer.23.attention.self.query.bias :  False\n",
      "encoder.layer.23.attention.self.key.weight :  False\n",
      "encoder.layer.23.attention.self.key.bias :  False\n",
      "encoder.layer.23.attention.self.value.weight :  False\n",
      "encoder.layer.23.attention.self.value.bias :  False\n",
      "encoder.layer.23.attention.output.dense.weight :  False\n",
      "encoder.layer.23.attention.output.dense.bias :  False\n",
      "encoder.layer.23.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.23.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.23.intermediate.dense.weight :  True\n",
      "encoder.layer.23.intermediate.dense.bias :  True\n",
      "encoder.layer.23.output.dense.weight :  True\n",
      "encoder.layer.23.output.dense.bias :  True\n",
      "encoder.layer.23.output.LayerNorm.weight :  False\n",
      "encoder.layer.23.output.LayerNorm.bias :  False\n",
      "pooler.dense.weight :  False\n",
      "pooler.dense.bias :  False\n"
     ]
    }
   ],
   "source": [
    "for name, param in auto_model.named_parameters():\n",
    "    print(name, \": \",param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses.MultipleNegativesRankingLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sbert.net/docs/package_reference/sentence_transformer/training_args.html#sentence_transformers.training_args.SentenceTransformerTrainingArguments\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"models/fine_tuned_gte\",\n",
    "\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=3, # default 3\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5, # default 5e-5\n",
    "    warmup_ratio=0.1, # Ratio of total training steps used for a linear warmup from 0 to learning_rate\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model = model,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=loss,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d981ede84ea4f1084e6faed40c00108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/852 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3433, 'grad_norm': 3.371968984603882, 'learning_rate': 9.26892950391645e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7bf9b2253d4f688caa7e0087384f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 143.8912, 'train_samples_per_second': 94.655, 'train_steps_per_second': 5.921, 'train_loss': 3.226212908964202, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=852, training_loss=3.226212908964202, metrics={'train_runtime': 143.8912, 'train_samples_per_second': 94.655, 'train_steps_per_second': 5.921, 'train_loss': 3.226212908964202, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./fine_tuned_gte_august11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
