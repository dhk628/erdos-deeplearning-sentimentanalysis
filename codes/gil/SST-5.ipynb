{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford Sentiment Treebank with 5 labels (SST-5)\n",
    "\n",
    "The SST-5, or Stanford Sentiment Treebank with 5 labels, is a dataset utilized for sentiment analysis. It contains 11,855 individual sentences sourced from movie reviews, along with 215,154 unique phrases from parse trees. These phrases are annotated by three human judges and are categorized as negative, somewhat negative, neutral, somewhat positive, or positive. This fine-grained labeling is what gives the dataset its name, SST-5.\n",
    "\n",
    "See https://paperswithcode.com/dataset/sst-5 for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S1.$ Storing data into text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytreebank # API version for SST-5 from https://pypi.org/project/pytreebank/\n",
    "import sys\n",
    "import os\n",
    "\n",
    "dataset = pytreebank.load_sst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3 (2 (2 The) (2 Rock) )(4 (3 (2 is) (4 (2 destined) (2 (2 (2 (2 (2 to) (2 (2 be) (2 (2 the) (2 (2 21st) (2 (2 (2 Century) (2 's) )(2 (3 new) (2 (2 ``) (2 Conan) )))))))(2 '') )(2 and) )(3 (2 that) (3 (2 he) (3 (2 's) (3 (2 going) (3 (2 to) (4 (3 (2 make) (3 (3 (2 a) (3 splash) )(2 (2 even) (3 greater) )))(2 (2 than) (2 (2 (2 (2 (1 (2 Arnold) (2 Schwarzenegger) )(2 ,) )(2 (2 Jean-Claud) (2 (2 Van) (2 Damme) )))(2 or) )(2 (2 Steven) (2 Segal) ))))))))))))(2 .) ))\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join('sst_{}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training data: 8544\n",
      "# of validation data: 1101\n",
      "# of test data: 2210\n"
     ]
    }
   ],
   "source": [
    "for category in ['train', 'test', 'dev']:\n",
    "    with open(out_path.format(category), 'w+') as outfile:\n",
    "        for item in dataset[category]:\n",
    "            outfile.write(\"__label__{}\\t{}\\n\".format(\n",
    "                item.to_labeled_lines()[0][0] + 1,\n",
    "                item.to_labeled_lines()[0][1]\n",
    "            ))\n",
    "# Print the length of the training set\n",
    "print(\"# of training data:\", len(dataset['train']))\n",
    "print(\"# of validation data:\", len(dataset['dev']))\n",
    "print(\"# of test data:\", len(dataset['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S2.$ Making data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8539</th>\n",
       "      <td>1</td>\n",
       "      <td>A real snooze .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8540</th>\n",
       "      <td>2</td>\n",
       "      <td>No surprises .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8541</th>\n",
       "      <td>4</td>\n",
       "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8542</th>\n",
       "      <td>1</td>\n",
       "      <td>Her fans walked out muttering words like `` ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8543</th>\n",
       "      <td>2</td>\n",
       "      <td>In this case zero .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8544 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0        4  The Rock is destined to be the 21st Century 's...\n",
       "1        5  The gorgeously elaborate continuation of `` Th...\n",
       "2        4  Singer/composer Bryan Adams contributes a slew...\n",
       "3        3  You 'd think by now America would have had eno...\n",
       "4        4               Yet the act is still charming here .\n",
       "...    ...                                                ...\n",
       "8539     1                                    A real snooze .\n",
       "8540     2                                     No surprises .\n",
       "8541     4  We 've seen the hippie-turned-yuppie plot befo...\n",
       "8542     1  Her fans walked out muttering words like `` ho...\n",
       "8543     2                                In this case zero .\n",
       "\n",
       "[8544 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"sst_train.txt\", sep=\"\\t\", header=None, names=['label','text'], encoding='latin-1')\n",
    "df_train['label'] = df_train['label'].str.replace(\"__label__\",\"\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>It 's a lovely film with lovely performances b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>No one goes unindicted here , which is probabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>And if you 're not nearly moved to tears by a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>A warm , funny , engaging film .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Uses sharp humor and insight into human nature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>2</td>\n",
       "      <td>it seems to me the film is about the art of ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>2</td>\n",
       "      <td>It 's just disappointingly superficial -- a mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>2</td>\n",
       "      <td>The title not only describes its main characte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>3</td>\n",
       "      <td>Sometimes it feels as if it might have been ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>2</td>\n",
       "      <td>Schaeffer has to find some hook on which to ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1101 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0        4  It 's a lovely film with lovely performances b...\n",
       "1        3  No one goes unindicted here , which is probabl...\n",
       "2        4  And if you 're not nearly moved to tears by a ...\n",
       "3        5                   A warm , funny , engaging film .\n",
       "4        5  Uses sharp humor and insight into human nature...\n",
       "...    ...                                                ...\n",
       "1096     2  it seems to me the film is about the art of ri...\n",
       "1097     2  It 's just disappointingly superficial -- a mo...\n",
       "1098     2  The title not only describes its main characte...\n",
       "1099     3  Sometimes it feels as if it might have been ma...\n",
       "1100     2  Schaeffer has to find some hook on which to ha...\n",
       "\n",
       "[1101 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev = pd.read_csv(\"sst_dev.txt\", sep=\"\\t\", header=None, names=['label','text'], encoding='latin-1')\n",
    "df_dev['label'] = df_dev['label'].str.replace(\"__label__\",\"\")\n",
    "df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The film provides some great insight into the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>4</td>\n",
       "      <td>An imaginative comedy/thriller .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>5</td>\n",
       "      <td>( A ) rare , beautiful film .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>5</td>\n",
       "      <td>( An ) hilarious romantic comedy .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>4</td>\n",
       "      <td>Never ( sinks ) into exploitation .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>1</td>\n",
       "      <td>( U ) nrelentingly stupid .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2210 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0        3                     Effective but too-tepid biopic\n",
       "1        4  If you sometimes like to go to the movies to h...\n",
       "2        5  Emerges as something rare , an issue movie tha...\n",
       "3        3  The film provides some great insight into the ...\n",
       "4        5  Offers that rare combination of entertainment ...\n",
       "...    ...                                                ...\n",
       "2205     4                   An imaginative comedy/thriller .\n",
       "2206     5                      ( A ) rare , beautiful film .\n",
       "2207     5                 ( An ) hilarious romantic comedy .\n",
       "2208     4                Never ( sinks ) into exploitation .\n",
       "2209     1                        ( U ) nrelentingly stupid .\n",
       "\n",
       "[2210 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"sst_test.txt\", sep=\"\\t\", header=None, names=['label','text'], encoding='latin-1')\n",
    "df_test['label'] = df_test['label'].str.replace(\"__label__\",\"\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S3.$ Vectorizing sentences using a pretrained GTE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gil\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that vectorizes a sentence\n",
    "\n",
    "def get_sentence_embedding(text, sentence_model):\n",
    "    if not text.strip(): \n",
    "    #.strip() gets rid of new lines\n",
    "        print(\"Attempted to get embedding for empty text.\")\n",
    "        return []\n",
    "\n",
    "    embedding = sentence_model.encode(text)\n",
    "\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pretrained GTE model for generating sentence embeddings\n",
    "gte_model = SentenceTransformer('thenlper/gte-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8544/8544 [02:45<00:00, 51.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>vector_gte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>[0.028988052159547806, -0.009446077048778534, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>[0.009347114711999893, -0.0242688599973917, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
       "      <td>[0.008711150847375393, 0.02659980021417141, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>[0.0290781632065773, -0.013503121212124825, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>[-0.009661804884672165, -0.008504939265549183,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8539</th>\n",
       "      <td>1</td>\n",
       "      <td>A real snooze .</td>\n",
       "      <td>[0.022056950256228447, 0.005160029046237469, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8540</th>\n",
       "      <td>2</td>\n",
       "      <td>No surprises .</td>\n",
       "      <td>[-0.02582346461713314, 0.01339962799102068, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8541</th>\n",
       "      <td>4</td>\n",
       "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>[0.022835902869701385, 0.005308559164404869, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8542</th>\n",
       "      <td>1</td>\n",
       "      <td>Her fans walked out muttering words like `` ho...</td>\n",
       "      <td>[0.0018498250283300877, -0.006132108625024557,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8543</th>\n",
       "      <td>2</td>\n",
       "      <td>In this case zero .</td>\n",
       "      <td>[-0.004530671983957291, 0.008977043442428112, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8544 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  \\\n",
       "0        4  The Rock is destined to be the 21st Century 's...   \n",
       "1        5  The gorgeously elaborate continuation of `` Th...   \n",
       "2        4  Singer/composer Bryan Adams contributes a slew...   \n",
       "3        3  You 'd think by now America would have had eno...   \n",
       "4        4               Yet the act is still charming here .   \n",
       "...    ...                                                ...   \n",
       "8539     1                                    A real snooze .   \n",
       "8540     2                                     No surprises .   \n",
       "8541     4  We 've seen the hippie-turned-yuppie plot befo...   \n",
       "8542     1  Her fans walked out muttering words like `` ho...   \n",
       "8543     2                                In this case zero .   \n",
       "\n",
       "                                             vector_gte  \n",
       "0     [0.028988052159547806, -0.009446077048778534, ...  \n",
       "1     [0.009347114711999893, -0.0242688599973917, 0....  \n",
       "2     [0.008711150847375393, 0.02659980021417141, 0....  \n",
       "3     [0.0290781632065773, -0.013503121212124825, -0...  \n",
       "4     [-0.009661804884672165, -0.008504939265549183,...  \n",
       "...                                                 ...  \n",
       "8539  [0.022056950256228447, 0.005160029046237469, -...  \n",
       "8540  [-0.02582346461713314, 0.01339962799102068, 0....  \n",
       "8541  [0.022835902869701385, 0.005308559164404869, -...  \n",
       "8542  [0.0018498250283300877, -0.006132108625024557,...  \n",
       "8543  [-0.004530671983957291, 0.008977043442428112, ...  \n",
       "\n",
       "[8544 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['vector_gte'] = df_train['text'].progress_apply(lambda x: get_sentence_embedding(x, gte_model))\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1101/1101 [00:25<00:00, 42.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>vector_gte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>It 's a lovely film with lovely performances b...</td>\n",
       "      <td>[0.014344191178679466, -0.0142461396753788, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>No one goes unindicted here , which is probabl...</td>\n",
       "      <td>[-0.03392713889479637, 0.004705153871327639, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>And if you 're not nearly moved to tears by a ...</td>\n",
       "      <td>[0.03554988279938698, 0.005494722165167332, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>A warm , funny , engaging film .</td>\n",
       "      <td>[0.0033094475511461496, -0.006415199488401413,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Uses sharp humor and insight into human nature...</td>\n",
       "      <td>[-0.003727895673364401, -0.01158248633146286, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>2</td>\n",
       "      <td>it seems to me the film is about the art of ri...</td>\n",
       "      <td>[-0.0041084568947553635, 0.0022037599701434374...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>2</td>\n",
       "      <td>It 's just disappointingly superficial -- a mo...</td>\n",
       "      <td>[0.02682664804160595, -0.0030683069489896297, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>2</td>\n",
       "      <td>The title not only describes its main characte...</td>\n",
       "      <td>[-0.01506032980978489, -0.01804324984550476, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>3</td>\n",
       "      <td>Sometimes it feels as if it might have been ma...</td>\n",
       "      <td>[0.01700528711080551, -0.019340755417943, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>2</td>\n",
       "      <td>Schaeffer has to find some hook on which to ha...</td>\n",
       "      <td>[0.018991904333233833, -0.027758155018091202, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1101 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  \\\n",
       "0        4  It 's a lovely film with lovely performances b...   \n",
       "1        3  No one goes unindicted here , which is probabl...   \n",
       "2        4  And if you 're not nearly moved to tears by a ...   \n",
       "3        5                   A warm , funny , engaging film .   \n",
       "4        5  Uses sharp humor and insight into human nature...   \n",
       "...    ...                                                ...   \n",
       "1096     2  it seems to me the film is about the art of ri...   \n",
       "1097     2  It 's just disappointingly superficial -- a mo...   \n",
       "1098     2  The title not only describes its main characte...   \n",
       "1099     3  Sometimes it feels as if it might have been ma...   \n",
       "1100     2  Schaeffer has to find some hook on which to ha...   \n",
       "\n",
       "                                             vector_gte  \n",
       "0     [0.014344191178679466, -0.0142461396753788, -0...  \n",
       "1     [-0.03392713889479637, 0.004705153871327639, -...  \n",
       "2     [0.03554988279938698, 0.005494722165167332, 0....  \n",
       "3     [0.0033094475511461496, -0.006415199488401413,...  \n",
       "4     [-0.003727895673364401, -0.01158248633146286, ...  \n",
       "...                                                 ...  \n",
       "1096  [-0.0041084568947553635, 0.0022037599701434374...  \n",
       "1097  [0.02682664804160595, -0.0030683069489896297, ...  \n",
       "1098  [-0.01506032980978489, -0.01804324984550476, -...  \n",
       "1099  [0.01700528711080551, -0.019340755417943, 0.01...  \n",
       "1100  [0.018991904333233833, -0.027758155018091202, ...  \n",
       "\n",
       "[1101 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['vector_gte'] = df_dev['text'].progress_apply(lambda x: get_sentence_embedding(x, gte_model))\n",
    "df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2210/2210 [00:41<00:00, 52.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>vector_gte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>[0.03771600499749184, -0.01407541148364544, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>[0.004801100119948387, 0.016335587948560715, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>[-0.007570610381662846, -0.014615904539823532,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The film provides some great insight into the ...</td>\n",
       "      <td>[0.0174684040248394, 0.0014129565097391605, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>[-0.0018031998770311475, 0.00950391124933958, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>4</td>\n",
       "      <td>An imaginative comedy/thriller .</td>\n",
       "      <td>[0.0037158015184104443, -0.017833540216088295,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>5</td>\n",
       "      <td>( A ) rare , beautiful film .</td>\n",
       "      <td>[0.014899299480021, -0.0012953567784279585, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>5</td>\n",
       "      <td>( An ) hilarious romantic comedy .</td>\n",
       "      <td>[0.00384555128403008, -0.028396954759955406, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>4</td>\n",
       "      <td>Never ( sinks ) into exploitation .</td>\n",
       "      <td>[-0.006038010120391846, 0.018880706280469894, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>1</td>\n",
       "      <td>( U ) nrelentingly stupid .</td>\n",
       "      <td>[0.008494336158037186, -0.0029037969652563334,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2210 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  \\\n",
       "0        3                     Effective but too-tepid biopic   \n",
       "1        4  If you sometimes like to go to the movies to h...   \n",
       "2        5  Emerges as something rare , an issue movie tha...   \n",
       "3        3  The film provides some great insight into the ...   \n",
       "4        5  Offers that rare combination of entertainment ...   \n",
       "...    ...                                                ...   \n",
       "2205     4                   An imaginative comedy/thriller .   \n",
       "2206     5                      ( A ) rare , beautiful film .   \n",
       "2207     5                 ( An ) hilarious romantic comedy .   \n",
       "2208     4                Never ( sinks ) into exploitation .   \n",
       "2209     1                        ( U ) nrelentingly stupid .   \n",
       "\n",
       "                                             vector_gte  \n",
       "0     [0.03771600499749184, -0.01407541148364544, -0...  \n",
       "1     [0.004801100119948387, 0.016335587948560715, -...  \n",
       "2     [-0.007570610381662846, -0.014615904539823532,...  \n",
       "3     [0.0174684040248394, 0.0014129565097391605, -0...  \n",
       "4     [-0.0018031998770311475, 0.00950391124933958, ...  \n",
       "...                                                 ...  \n",
       "2205  [0.0037158015184104443, -0.017833540216088295,...  \n",
       "2206  [0.014899299480021, -0.0012953567784279585, -0...  \n",
       "2207  [0.00384555128403008, -0.028396954759955406, -...  \n",
       "2208  [-0.006038010120391846, 0.018880706280469894, ...  \n",
       "2209  [0.008494336158037186, -0.0029037969652563334,...  \n",
       "\n",
       "[2210 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['vector_gte'] = df_test['text'].progress_apply(lambda x: get_sentence_embedding(x, gte_model))\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_parquet(\"SST-5_train_gte.parquet\")\n",
    "df_dev.to_parquet(\"SST-5_validation_gte.parquet\")\n",
    "df_test.to_parquet(\"SST-5_test_gte.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**. If you have run the codes above before, run the following instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(\"SST-5_train_gte.parquet\")\n",
    "df_dev = pd.read_parquet(\"SST-5_validation_gte.parquet\")\n",
    "df_test = pd.read_parquet(\"SST-5_test_gte.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['label'] = df_train['label'].astype(int)\n",
    "df_dev['label'] = df_dev['label'].astype(int)\n",
    "df_test['label'] = df_test['label'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S4.$ Fine-tuning pretrained GTE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts[0] contains label 1\n",
    "# ...\n",
    "# texts[4] contains label 5\n",
    "\n",
    "texts = []\n",
    "\n",
    "for i in range(5):\n",
    "    texts.append( df_train[df_train['label'] == i+1]['text'].tolist() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of 1-star ratings: 1092\n",
      "# of 2-star ratings: 2218\n",
      "# of 3-star ratings: 1624\n",
      "# of 4-star ratings: 2322\n",
      "# of 5-star ratings: 1288\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"# of {i+1}-star ratings:\", len(texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100) # setting the random seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    sampled_texts.append(random.sample(texts[i], 1092))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1092\n",
      "1092\n",
      "1092\n",
      "1092\n",
      "1092\n"
     ]
    }
   ],
   "source": [
    "for s in sampled_texts:\n",
    "    print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import concatenate_datasets # https://huggingface.co/docs/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating anchor-positive dataset that separates rating 5 from rating 1\n",
    "# 1092 / 2 = 546\n",
    "\n",
    "dataset_5_from_1_first = Dataset.from_dict({\n",
    "    'anchor': sampled_texts[4][:546], # index 4 means rating 5\n",
    "    'positive': sampled_texts[4][546:],\n",
    "    'negative': sampled_texts[0][:546] # index 0 means rating 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5_from_1_second = Dataset.from_dict({\n",
    "    'anchor': sampled_texts[4][:546],\n",
    "    'positive': sampled_texts[4][546:],\n",
    "    'negative': sampled_texts[0][546:]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating anchor-positive dataset that separates rating 5 from rating 2\n",
    "# 1092 / 2 = 546\n",
    "\n",
    "dataset_5_from_2_first = Dataset.from_dict({\n",
    "    'anchor': sampled_texts[4][:546],\n",
    "    'positive': sampled_texts[4][546:],\n",
    "    'negative': sampled_texts[1][:546] # idx 1 means rating 2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5_from_2_second = Dataset.from_dict({\n",
    "    'anchor': sampled_texts[4][:546],\n",
    "    'positive': sampled_texts[4][546:],\n",
    "    'negative': sampled_texts[1][546:] # idx 1 means rating 2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating anchor-positive dataset that separates rating 1 from rating 5\n",
    "# 1092 / 2 = 546\n",
    "\n",
    "dataset_1_from_5_first = Dataset.from_dict({\n",
    "    'anchor': sampled_texts[0][:546],\n",
    "    'positive': sampled_texts[0][546:],\n",
    "    'negative': sampled_texts[4][:546]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_from_5_second = Dataset.from_dict({\n",
    "    'anchor': sampled_texts[0][:546],\n",
    "    'positive': sampled_texts[0][546:],\n",
    "    'negative': sampled_texts[4][546:]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating anchor-positive dataset that separates rating 1 from rating 4\n",
    "# 1092 / 2 = 546\n",
    "\n",
    "dataset_1_from_4_first = Dataset.from_dict({\n",
    "    'anchor': sampled_texts[0][:546],\n",
    "    'positive': sampled_texts[0][546:],\n",
    "    'negative': sampled_texts[3][:546]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_from_4_second = Dataset.from_dict({\n",
    "    'anchor': sampled_texts[0][:546],\n",
    "    'positive': sampled_texts[0][546:],\n",
    "    'negative': sampled_texts[3][546:]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate datasets -- https://huggingface.co/docs/datasets/v1.3.0/processing.html\n",
    "\n",
    "train_dataset = concatenate_datasets(\n",
    "    [\n",
    "        dataset_5_from_1_first,\n",
    "        dataset_5_from_1_second,\n",
    "        dataset_5_from_2_first,\n",
    "        dataset_5_from_2_second,\n",
    "        dataset_1_from_5_first,\n",
    "        dataset_1_from_5_second,\n",
    "        dataset_1_from_4_first,\n",
    "        dataset_1_from_4_second\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gte_model._first_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 1024)\n",
       "    (token_type_embeddings): Embedding(2, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_model = gte_model._first_module().auto_model\n",
    "auto_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "encoder.layer.12.attention.self.query.weight\n",
      "encoder.layer.12.attention.self.query.bias\n",
      "encoder.layer.12.attention.self.key.weight\n",
      "encoder.layer.12.attention.self.key.bias\n",
      "encoder.layer.12.attention.self.value.weight\n",
      "encoder.layer.12.attention.self.value.bias\n",
      "encoder.layer.12.attention.output.dense.weight\n",
      "encoder.layer.12.attention.output.dense.bias\n",
      "encoder.layer.12.attention.output.LayerNorm.weight\n",
      "encoder.layer.12.attention.output.LayerNorm.bias\n",
      "encoder.layer.12.intermediate.dense.weight\n",
      "encoder.layer.12.intermediate.dense.bias\n",
      "encoder.layer.12.output.dense.weight\n",
      "encoder.layer.12.output.dense.bias\n",
      "encoder.layer.12.output.LayerNorm.weight\n",
      "encoder.layer.12.output.LayerNorm.bias\n",
      "encoder.layer.13.attention.self.query.weight\n",
      "encoder.layer.13.attention.self.query.bias\n",
      "encoder.layer.13.attention.self.key.weight\n",
      "encoder.layer.13.attention.self.key.bias\n",
      "encoder.layer.13.attention.self.value.weight\n",
      "encoder.layer.13.attention.self.value.bias\n",
      "encoder.layer.13.attention.output.dense.weight\n",
      "encoder.layer.13.attention.output.dense.bias\n",
      "encoder.layer.13.attention.output.LayerNorm.weight\n",
      "encoder.layer.13.attention.output.LayerNorm.bias\n",
      "encoder.layer.13.intermediate.dense.weight\n",
      "encoder.layer.13.intermediate.dense.bias\n",
      "encoder.layer.13.output.dense.weight\n",
      "encoder.layer.13.output.dense.bias\n",
      "encoder.layer.13.output.LayerNorm.weight\n",
      "encoder.layer.13.output.LayerNorm.bias\n",
      "encoder.layer.14.attention.self.query.weight\n",
      "encoder.layer.14.attention.self.query.bias\n",
      "encoder.layer.14.attention.self.key.weight\n",
      "encoder.layer.14.attention.self.key.bias\n",
      "encoder.layer.14.attention.self.value.weight\n",
      "encoder.layer.14.attention.self.value.bias\n",
      "encoder.layer.14.attention.output.dense.weight\n",
      "encoder.layer.14.attention.output.dense.bias\n",
      "encoder.layer.14.attention.output.LayerNorm.weight\n",
      "encoder.layer.14.attention.output.LayerNorm.bias\n",
      "encoder.layer.14.intermediate.dense.weight\n",
      "encoder.layer.14.intermediate.dense.bias\n",
      "encoder.layer.14.output.dense.weight\n",
      "encoder.layer.14.output.dense.bias\n",
      "encoder.layer.14.output.LayerNorm.weight\n",
      "encoder.layer.14.output.LayerNorm.bias\n",
      "encoder.layer.15.attention.self.query.weight\n",
      "encoder.layer.15.attention.self.query.bias\n",
      "encoder.layer.15.attention.self.key.weight\n",
      "encoder.layer.15.attention.self.key.bias\n",
      "encoder.layer.15.attention.self.value.weight\n",
      "encoder.layer.15.attention.self.value.bias\n",
      "encoder.layer.15.attention.output.dense.weight\n",
      "encoder.layer.15.attention.output.dense.bias\n",
      "encoder.layer.15.attention.output.LayerNorm.weight\n",
      "encoder.layer.15.attention.output.LayerNorm.bias\n",
      "encoder.layer.15.intermediate.dense.weight\n",
      "encoder.layer.15.intermediate.dense.bias\n",
      "encoder.layer.15.output.dense.weight\n",
      "encoder.layer.15.output.dense.bias\n",
      "encoder.layer.15.output.LayerNorm.weight\n",
      "encoder.layer.15.output.LayerNorm.bias\n",
      "encoder.layer.16.attention.self.query.weight\n",
      "encoder.layer.16.attention.self.query.bias\n",
      "encoder.layer.16.attention.self.key.weight\n",
      "encoder.layer.16.attention.self.key.bias\n",
      "encoder.layer.16.attention.self.value.weight\n",
      "encoder.layer.16.attention.self.value.bias\n",
      "encoder.layer.16.attention.output.dense.weight\n",
      "encoder.layer.16.attention.output.dense.bias\n",
      "encoder.layer.16.attention.output.LayerNorm.weight\n",
      "encoder.layer.16.attention.output.LayerNorm.bias\n",
      "encoder.layer.16.intermediate.dense.weight\n",
      "encoder.layer.16.intermediate.dense.bias\n",
      "encoder.layer.16.output.dense.weight\n",
      "encoder.layer.16.output.dense.bias\n",
      "encoder.layer.16.output.LayerNorm.weight\n",
      "encoder.layer.16.output.LayerNorm.bias\n",
      "encoder.layer.17.attention.self.query.weight\n",
      "encoder.layer.17.attention.self.query.bias\n",
      "encoder.layer.17.attention.self.key.weight\n",
      "encoder.layer.17.attention.self.key.bias\n",
      "encoder.layer.17.attention.self.value.weight\n",
      "encoder.layer.17.attention.self.value.bias\n",
      "encoder.layer.17.attention.output.dense.weight\n",
      "encoder.layer.17.attention.output.dense.bias\n",
      "encoder.layer.17.attention.output.LayerNorm.weight\n",
      "encoder.layer.17.attention.output.LayerNorm.bias\n",
      "encoder.layer.17.intermediate.dense.weight\n",
      "encoder.layer.17.intermediate.dense.bias\n",
      "encoder.layer.17.output.dense.weight\n",
      "encoder.layer.17.output.dense.bias\n",
      "encoder.layer.17.output.LayerNorm.weight\n",
      "encoder.layer.17.output.LayerNorm.bias\n",
      "encoder.layer.18.attention.self.query.weight\n",
      "encoder.layer.18.attention.self.query.bias\n",
      "encoder.layer.18.attention.self.key.weight\n",
      "encoder.layer.18.attention.self.key.bias\n",
      "encoder.layer.18.attention.self.value.weight\n",
      "encoder.layer.18.attention.self.value.bias\n",
      "encoder.layer.18.attention.output.dense.weight\n",
      "encoder.layer.18.attention.output.dense.bias\n",
      "encoder.layer.18.attention.output.LayerNorm.weight\n",
      "encoder.layer.18.attention.output.LayerNorm.bias\n",
      "encoder.layer.18.intermediate.dense.weight\n",
      "encoder.layer.18.intermediate.dense.bias\n",
      "encoder.layer.18.output.dense.weight\n",
      "encoder.layer.18.output.dense.bias\n",
      "encoder.layer.18.output.LayerNorm.weight\n",
      "encoder.layer.18.output.LayerNorm.bias\n",
      "encoder.layer.19.attention.self.query.weight\n",
      "encoder.layer.19.attention.self.query.bias\n",
      "encoder.layer.19.attention.self.key.weight\n",
      "encoder.layer.19.attention.self.key.bias\n",
      "encoder.layer.19.attention.self.value.weight\n",
      "encoder.layer.19.attention.self.value.bias\n",
      "encoder.layer.19.attention.output.dense.weight\n",
      "encoder.layer.19.attention.output.dense.bias\n",
      "encoder.layer.19.attention.output.LayerNorm.weight\n",
      "encoder.layer.19.attention.output.LayerNorm.bias\n",
      "encoder.layer.19.intermediate.dense.weight\n",
      "encoder.layer.19.intermediate.dense.bias\n",
      "encoder.layer.19.output.dense.weight\n",
      "encoder.layer.19.output.dense.bias\n",
      "encoder.layer.19.output.LayerNorm.weight\n",
      "encoder.layer.19.output.LayerNorm.bias\n",
      "encoder.layer.20.attention.self.query.weight\n",
      "encoder.layer.20.attention.self.query.bias\n",
      "encoder.layer.20.attention.self.key.weight\n",
      "encoder.layer.20.attention.self.key.bias\n",
      "encoder.layer.20.attention.self.value.weight\n",
      "encoder.layer.20.attention.self.value.bias\n",
      "encoder.layer.20.attention.output.dense.weight\n",
      "encoder.layer.20.attention.output.dense.bias\n",
      "encoder.layer.20.attention.output.LayerNorm.weight\n",
      "encoder.layer.20.attention.output.LayerNorm.bias\n",
      "encoder.layer.20.intermediate.dense.weight\n",
      "encoder.layer.20.intermediate.dense.bias\n",
      "encoder.layer.20.output.dense.weight\n",
      "encoder.layer.20.output.dense.bias\n",
      "encoder.layer.20.output.LayerNorm.weight\n",
      "encoder.layer.20.output.LayerNorm.bias\n",
      "encoder.layer.21.attention.self.query.weight\n",
      "encoder.layer.21.attention.self.query.bias\n",
      "encoder.layer.21.attention.self.key.weight\n",
      "encoder.layer.21.attention.self.key.bias\n",
      "encoder.layer.21.attention.self.value.weight\n",
      "encoder.layer.21.attention.self.value.bias\n",
      "encoder.layer.21.attention.output.dense.weight\n",
      "encoder.layer.21.attention.output.dense.bias\n",
      "encoder.layer.21.attention.output.LayerNorm.weight\n",
      "encoder.layer.21.attention.output.LayerNorm.bias\n",
      "encoder.layer.21.intermediate.dense.weight\n",
      "encoder.layer.21.intermediate.dense.bias\n",
      "encoder.layer.21.output.dense.weight\n",
      "encoder.layer.21.output.dense.bias\n",
      "encoder.layer.21.output.LayerNorm.weight\n",
      "encoder.layer.21.output.LayerNorm.bias\n",
      "encoder.layer.22.attention.self.query.weight\n",
      "encoder.layer.22.attention.self.query.bias\n",
      "encoder.layer.22.attention.self.key.weight\n",
      "encoder.layer.22.attention.self.key.bias\n",
      "encoder.layer.22.attention.self.value.weight\n",
      "encoder.layer.22.attention.self.value.bias\n",
      "encoder.layer.22.attention.output.dense.weight\n",
      "encoder.layer.22.attention.output.dense.bias\n",
      "encoder.layer.22.attention.output.LayerNorm.weight\n",
      "encoder.layer.22.attention.output.LayerNorm.bias\n",
      "encoder.layer.22.intermediate.dense.weight\n",
      "encoder.layer.22.intermediate.dense.bias\n",
      "encoder.layer.22.output.dense.weight\n",
      "encoder.layer.22.output.dense.bias\n",
      "encoder.layer.22.output.LayerNorm.weight\n",
      "encoder.layer.22.output.LayerNorm.bias\n",
      "encoder.layer.23.attention.self.query.weight\n",
      "encoder.layer.23.attention.self.query.bias\n",
      "encoder.layer.23.attention.self.key.weight\n",
      "encoder.layer.23.attention.self.key.bias\n",
      "encoder.layer.23.attention.self.value.weight\n",
      "encoder.layer.23.attention.self.value.bias\n",
      "encoder.layer.23.attention.output.dense.weight\n",
      "encoder.layer.23.attention.output.dense.bias\n",
      "encoder.layer.23.attention.output.LayerNorm.weight\n",
      "encoder.layer.23.attention.output.LayerNorm.bias\n",
      "encoder.layer.23.intermediate.dense.weight\n",
      "encoder.layer.23.intermediate.dense.bias\n",
      "encoder.layer.23.output.dense.weight\n",
      "encoder.layer.23.output.dense.bias\n",
      "encoder.layer.23.output.LayerNorm.weight\n",
      "encoder.layer.23.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in auto_model.named_parameters():\n",
    "    print(name)\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in auto_model.named_parameters():\n",
    "    for i in range(20, 24):\n",
    "        if name == f'encoder.layer.{i}.output.dense.weight':\n",
    "            param.requires_grad = True\n",
    "        elif name == f'encoder.layer.{i}.output.dense.bias':\n",
    "            param.requires_grad = True\n",
    "        elif name == f'encoder.layer.{i}.intermediate.dense.weight':\n",
    "            param.requires_grad = True\n",
    "        elif name == f'encoder.layer.{i}.intermediate.dense.bias':\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight :  False\n",
      "embeddings.position_embeddings.weight :  False\n",
      "embeddings.token_type_embeddings.weight :  False\n",
      "embeddings.LayerNorm.weight :  False\n",
      "embeddings.LayerNorm.bias :  False\n",
      "encoder.layer.0.attention.self.query.weight :  False\n",
      "encoder.layer.0.attention.self.query.bias :  False\n",
      "encoder.layer.0.attention.self.key.weight :  False\n",
      "encoder.layer.0.attention.self.key.bias :  False\n",
      "encoder.layer.0.attention.self.value.weight :  False\n",
      "encoder.layer.0.attention.self.value.bias :  False\n",
      "encoder.layer.0.attention.output.dense.weight :  False\n",
      "encoder.layer.0.attention.output.dense.bias :  False\n",
      "encoder.layer.0.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.0.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.0.intermediate.dense.weight :  False\n",
      "encoder.layer.0.intermediate.dense.bias :  False\n",
      "encoder.layer.0.output.dense.weight :  False\n",
      "encoder.layer.0.output.dense.bias :  False\n",
      "encoder.layer.0.output.LayerNorm.weight :  False\n",
      "encoder.layer.0.output.LayerNorm.bias :  False\n",
      "encoder.layer.1.attention.self.query.weight :  False\n",
      "encoder.layer.1.attention.self.query.bias :  False\n",
      "encoder.layer.1.attention.self.key.weight :  False\n",
      "encoder.layer.1.attention.self.key.bias :  False\n",
      "encoder.layer.1.attention.self.value.weight :  False\n",
      "encoder.layer.1.attention.self.value.bias :  False\n",
      "encoder.layer.1.attention.output.dense.weight :  False\n",
      "encoder.layer.1.attention.output.dense.bias :  False\n",
      "encoder.layer.1.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.1.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.1.intermediate.dense.weight :  False\n",
      "encoder.layer.1.intermediate.dense.bias :  False\n",
      "encoder.layer.1.output.dense.weight :  False\n",
      "encoder.layer.1.output.dense.bias :  False\n",
      "encoder.layer.1.output.LayerNorm.weight :  False\n",
      "encoder.layer.1.output.LayerNorm.bias :  False\n",
      "encoder.layer.2.attention.self.query.weight :  False\n",
      "encoder.layer.2.attention.self.query.bias :  False\n",
      "encoder.layer.2.attention.self.key.weight :  False\n",
      "encoder.layer.2.attention.self.key.bias :  False\n",
      "encoder.layer.2.attention.self.value.weight :  False\n",
      "encoder.layer.2.attention.self.value.bias :  False\n",
      "encoder.layer.2.attention.output.dense.weight :  False\n",
      "encoder.layer.2.attention.output.dense.bias :  False\n",
      "encoder.layer.2.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.2.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.2.intermediate.dense.weight :  False\n",
      "encoder.layer.2.intermediate.dense.bias :  False\n",
      "encoder.layer.2.output.dense.weight :  False\n",
      "encoder.layer.2.output.dense.bias :  False\n",
      "encoder.layer.2.output.LayerNorm.weight :  False\n",
      "encoder.layer.2.output.LayerNorm.bias :  False\n",
      "encoder.layer.3.attention.self.query.weight :  False\n",
      "encoder.layer.3.attention.self.query.bias :  False\n",
      "encoder.layer.3.attention.self.key.weight :  False\n",
      "encoder.layer.3.attention.self.key.bias :  False\n",
      "encoder.layer.3.attention.self.value.weight :  False\n",
      "encoder.layer.3.attention.self.value.bias :  False\n",
      "encoder.layer.3.attention.output.dense.weight :  False\n",
      "encoder.layer.3.attention.output.dense.bias :  False\n",
      "encoder.layer.3.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.3.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.3.intermediate.dense.weight :  False\n",
      "encoder.layer.3.intermediate.dense.bias :  False\n",
      "encoder.layer.3.output.dense.weight :  False\n",
      "encoder.layer.3.output.dense.bias :  False\n",
      "encoder.layer.3.output.LayerNorm.weight :  False\n",
      "encoder.layer.3.output.LayerNorm.bias :  False\n",
      "encoder.layer.4.attention.self.query.weight :  False\n",
      "encoder.layer.4.attention.self.query.bias :  False\n",
      "encoder.layer.4.attention.self.key.weight :  False\n",
      "encoder.layer.4.attention.self.key.bias :  False\n",
      "encoder.layer.4.attention.self.value.weight :  False\n",
      "encoder.layer.4.attention.self.value.bias :  False\n",
      "encoder.layer.4.attention.output.dense.weight :  False\n",
      "encoder.layer.4.attention.output.dense.bias :  False\n",
      "encoder.layer.4.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.4.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.4.intermediate.dense.weight :  False\n",
      "encoder.layer.4.intermediate.dense.bias :  False\n",
      "encoder.layer.4.output.dense.weight :  False\n",
      "encoder.layer.4.output.dense.bias :  False\n",
      "encoder.layer.4.output.LayerNorm.weight :  False\n",
      "encoder.layer.4.output.LayerNorm.bias :  False\n",
      "encoder.layer.5.attention.self.query.weight :  False\n",
      "encoder.layer.5.attention.self.query.bias :  False\n",
      "encoder.layer.5.attention.self.key.weight :  False\n",
      "encoder.layer.5.attention.self.key.bias :  False\n",
      "encoder.layer.5.attention.self.value.weight :  False\n",
      "encoder.layer.5.attention.self.value.bias :  False\n",
      "encoder.layer.5.attention.output.dense.weight :  False\n",
      "encoder.layer.5.attention.output.dense.bias :  False\n",
      "encoder.layer.5.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.5.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.5.intermediate.dense.weight :  False\n",
      "encoder.layer.5.intermediate.dense.bias :  False\n",
      "encoder.layer.5.output.dense.weight :  False\n",
      "encoder.layer.5.output.dense.bias :  False\n",
      "encoder.layer.5.output.LayerNorm.weight :  False\n",
      "encoder.layer.5.output.LayerNorm.bias :  False\n",
      "encoder.layer.6.attention.self.query.weight :  False\n",
      "encoder.layer.6.attention.self.query.bias :  False\n",
      "encoder.layer.6.attention.self.key.weight :  False\n",
      "encoder.layer.6.attention.self.key.bias :  False\n",
      "encoder.layer.6.attention.self.value.weight :  False\n",
      "encoder.layer.6.attention.self.value.bias :  False\n",
      "encoder.layer.6.attention.output.dense.weight :  False\n",
      "encoder.layer.6.attention.output.dense.bias :  False\n",
      "encoder.layer.6.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.6.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.6.intermediate.dense.weight :  False\n",
      "encoder.layer.6.intermediate.dense.bias :  False\n",
      "encoder.layer.6.output.dense.weight :  False\n",
      "encoder.layer.6.output.dense.bias :  False\n",
      "encoder.layer.6.output.LayerNorm.weight :  False\n",
      "encoder.layer.6.output.LayerNorm.bias :  False\n",
      "encoder.layer.7.attention.self.query.weight :  False\n",
      "encoder.layer.7.attention.self.query.bias :  False\n",
      "encoder.layer.7.attention.self.key.weight :  False\n",
      "encoder.layer.7.attention.self.key.bias :  False\n",
      "encoder.layer.7.attention.self.value.weight :  False\n",
      "encoder.layer.7.attention.self.value.bias :  False\n",
      "encoder.layer.7.attention.output.dense.weight :  False\n",
      "encoder.layer.7.attention.output.dense.bias :  False\n",
      "encoder.layer.7.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.7.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.7.intermediate.dense.weight :  False\n",
      "encoder.layer.7.intermediate.dense.bias :  False\n",
      "encoder.layer.7.output.dense.weight :  False\n",
      "encoder.layer.7.output.dense.bias :  False\n",
      "encoder.layer.7.output.LayerNorm.weight :  False\n",
      "encoder.layer.7.output.LayerNorm.bias :  False\n",
      "encoder.layer.8.attention.self.query.weight :  False\n",
      "encoder.layer.8.attention.self.query.bias :  False\n",
      "encoder.layer.8.attention.self.key.weight :  False\n",
      "encoder.layer.8.attention.self.key.bias :  False\n",
      "encoder.layer.8.attention.self.value.weight :  False\n",
      "encoder.layer.8.attention.self.value.bias :  False\n",
      "encoder.layer.8.attention.output.dense.weight :  False\n",
      "encoder.layer.8.attention.output.dense.bias :  False\n",
      "encoder.layer.8.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.8.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.8.intermediate.dense.weight :  False\n",
      "encoder.layer.8.intermediate.dense.bias :  False\n",
      "encoder.layer.8.output.dense.weight :  False\n",
      "encoder.layer.8.output.dense.bias :  False\n",
      "encoder.layer.8.output.LayerNorm.weight :  False\n",
      "encoder.layer.8.output.LayerNorm.bias :  False\n",
      "encoder.layer.9.attention.self.query.weight :  False\n",
      "encoder.layer.9.attention.self.query.bias :  False\n",
      "encoder.layer.9.attention.self.key.weight :  False\n",
      "encoder.layer.9.attention.self.key.bias :  False\n",
      "encoder.layer.9.attention.self.value.weight :  False\n",
      "encoder.layer.9.attention.self.value.bias :  False\n",
      "encoder.layer.9.attention.output.dense.weight :  False\n",
      "encoder.layer.9.attention.output.dense.bias :  False\n",
      "encoder.layer.9.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.9.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.9.intermediate.dense.weight :  False\n",
      "encoder.layer.9.intermediate.dense.bias :  False\n",
      "encoder.layer.9.output.dense.weight :  False\n",
      "encoder.layer.9.output.dense.bias :  False\n",
      "encoder.layer.9.output.LayerNorm.weight :  False\n",
      "encoder.layer.9.output.LayerNorm.bias :  False\n",
      "encoder.layer.10.attention.self.query.weight :  False\n",
      "encoder.layer.10.attention.self.query.bias :  False\n",
      "encoder.layer.10.attention.self.key.weight :  False\n",
      "encoder.layer.10.attention.self.key.bias :  False\n",
      "encoder.layer.10.attention.self.value.weight :  False\n",
      "encoder.layer.10.attention.self.value.bias :  False\n",
      "encoder.layer.10.attention.output.dense.weight :  False\n",
      "encoder.layer.10.attention.output.dense.bias :  False\n",
      "encoder.layer.10.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.10.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.10.intermediate.dense.weight :  False\n",
      "encoder.layer.10.intermediate.dense.bias :  False\n",
      "encoder.layer.10.output.dense.weight :  False\n",
      "encoder.layer.10.output.dense.bias :  False\n",
      "encoder.layer.10.output.LayerNorm.weight :  False\n",
      "encoder.layer.10.output.LayerNorm.bias :  False\n",
      "encoder.layer.11.attention.self.query.weight :  False\n",
      "encoder.layer.11.attention.self.query.bias :  False\n",
      "encoder.layer.11.attention.self.key.weight :  False\n",
      "encoder.layer.11.attention.self.key.bias :  False\n",
      "encoder.layer.11.attention.self.value.weight :  False\n",
      "encoder.layer.11.attention.self.value.bias :  False\n",
      "encoder.layer.11.attention.output.dense.weight :  False\n",
      "encoder.layer.11.attention.output.dense.bias :  False\n",
      "encoder.layer.11.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.11.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.11.intermediate.dense.weight :  False\n",
      "encoder.layer.11.intermediate.dense.bias :  False\n",
      "encoder.layer.11.output.dense.weight :  False\n",
      "encoder.layer.11.output.dense.bias :  False\n",
      "encoder.layer.11.output.LayerNorm.weight :  False\n",
      "encoder.layer.11.output.LayerNorm.bias :  False\n",
      "encoder.layer.12.attention.self.query.weight :  False\n",
      "encoder.layer.12.attention.self.query.bias :  False\n",
      "encoder.layer.12.attention.self.key.weight :  False\n",
      "encoder.layer.12.attention.self.key.bias :  False\n",
      "encoder.layer.12.attention.self.value.weight :  False\n",
      "encoder.layer.12.attention.self.value.bias :  False\n",
      "encoder.layer.12.attention.output.dense.weight :  False\n",
      "encoder.layer.12.attention.output.dense.bias :  False\n",
      "encoder.layer.12.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.12.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.12.intermediate.dense.weight :  False\n",
      "encoder.layer.12.intermediate.dense.bias :  False\n",
      "encoder.layer.12.output.dense.weight :  False\n",
      "encoder.layer.12.output.dense.bias :  False\n",
      "encoder.layer.12.output.LayerNorm.weight :  False\n",
      "encoder.layer.12.output.LayerNorm.bias :  False\n",
      "encoder.layer.13.attention.self.query.weight :  False\n",
      "encoder.layer.13.attention.self.query.bias :  False\n",
      "encoder.layer.13.attention.self.key.weight :  False\n",
      "encoder.layer.13.attention.self.key.bias :  False\n",
      "encoder.layer.13.attention.self.value.weight :  False\n",
      "encoder.layer.13.attention.self.value.bias :  False\n",
      "encoder.layer.13.attention.output.dense.weight :  False\n",
      "encoder.layer.13.attention.output.dense.bias :  False\n",
      "encoder.layer.13.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.13.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.13.intermediate.dense.weight :  False\n",
      "encoder.layer.13.intermediate.dense.bias :  False\n",
      "encoder.layer.13.output.dense.weight :  False\n",
      "encoder.layer.13.output.dense.bias :  False\n",
      "encoder.layer.13.output.LayerNorm.weight :  False\n",
      "encoder.layer.13.output.LayerNorm.bias :  False\n",
      "encoder.layer.14.attention.self.query.weight :  False\n",
      "encoder.layer.14.attention.self.query.bias :  False\n",
      "encoder.layer.14.attention.self.key.weight :  False\n",
      "encoder.layer.14.attention.self.key.bias :  False\n",
      "encoder.layer.14.attention.self.value.weight :  False\n",
      "encoder.layer.14.attention.self.value.bias :  False\n",
      "encoder.layer.14.attention.output.dense.weight :  False\n",
      "encoder.layer.14.attention.output.dense.bias :  False\n",
      "encoder.layer.14.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.14.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.14.intermediate.dense.weight :  False\n",
      "encoder.layer.14.intermediate.dense.bias :  False\n",
      "encoder.layer.14.output.dense.weight :  False\n",
      "encoder.layer.14.output.dense.bias :  False\n",
      "encoder.layer.14.output.LayerNorm.weight :  False\n",
      "encoder.layer.14.output.LayerNorm.bias :  False\n",
      "encoder.layer.15.attention.self.query.weight :  False\n",
      "encoder.layer.15.attention.self.query.bias :  False\n",
      "encoder.layer.15.attention.self.key.weight :  False\n",
      "encoder.layer.15.attention.self.key.bias :  False\n",
      "encoder.layer.15.attention.self.value.weight :  False\n",
      "encoder.layer.15.attention.self.value.bias :  False\n",
      "encoder.layer.15.attention.output.dense.weight :  False\n",
      "encoder.layer.15.attention.output.dense.bias :  False\n",
      "encoder.layer.15.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.15.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.15.intermediate.dense.weight :  False\n",
      "encoder.layer.15.intermediate.dense.bias :  False\n",
      "encoder.layer.15.output.dense.weight :  False\n",
      "encoder.layer.15.output.dense.bias :  False\n",
      "encoder.layer.15.output.LayerNorm.weight :  False\n",
      "encoder.layer.15.output.LayerNorm.bias :  False\n",
      "encoder.layer.16.attention.self.query.weight :  False\n",
      "encoder.layer.16.attention.self.query.bias :  False\n",
      "encoder.layer.16.attention.self.key.weight :  False\n",
      "encoder.layer.16.attention.self.key.bias :  False\n",
      "encoder.layer.16.attention.self.value.weight :  False\n",
      "encoder.layer.16.attention.self.value.bias :  False\n",
      "encoder.layer.16.attention.output.dense.weight :  False\n",
      "encoder.layer.16.attention.output.dense.bias :  False\n",
      "encoder.layer.16.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.16.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.16.intermediate.dense.weight :  False\n",
      "encoder.layer.16.intermediate.dense.bias :  False\n",
      "encoder.layer.16.output.dense.weight :  False\n",
      "encoder.layer.16.output.dense.bias :  False\n",
      "encoder.layer.16.output.LayerNorm.weight :  False\n",
      "encoder.layer.16.output.LayerNorm.bias :  False\n",
      "encoder.layer.17.attention.self.query.weight :  False\n",
      "encoder.layer.17.attention.self.query.bias :  False\n",
      "encoder.layer.17.attention.self.key.weight :  False\n",
      "encoder.layer.17.attention.self.key.bias :  False\n",
      "encoder.layer.17.attention.self.value.weight :  False\n",
      "encoder.layer.17.attention.self.value.bias :  False\n",
      "encoder.layer.17.attention.output.dense.weight :  False\n",
      "encoder.layer.17.attention.output.dense.bias :  False\n",
      "encoder.layer.17.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.17.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.17.intermediate.dense.weight :  False\n",
      "encoder.layer.17.intermediate.dense.bias :  False\n",
      "encoder.layer.17.output.dense.weight :  False\n",
      "encoder.layer.17.output.dense.bias :  False\n",
      "encoder.layer.17.output.LayerNorm.weight :  False\n",
      "encoder.layer.17.output.LayerNorm.bias :  False\n",
      "encoder.layer.18.attention.self.query.weight :  False\n",
      "encoder.layer.18.attention.self.query.bias :  False\n",
      "encoder.layer.18.attention.self.key.weight :  False\n",
      "encoder.layer.18.attention.self.key.bias :  False\n",
      "encoder.layer.18.attention.self.value.weight :  False\n",
      "encoder.layer.18.attention.self.value.bias :  False\n",
      "encoder.layer.18.attention.output.dense.weight :  False\n",
      "encoder.layer.18.attention.output.dense.bias :  False\n",
      "encoder.layer.18.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.18.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.18.intermediate.dense.weight :  False\n",
      "encoder.layer.18.intermediate.dense.bias :  False\n",
      "encoder.layer.18.output.dense.weight :  False\n",
      "encoder.layer.18.output.dense.bias :  False\n",
      "encoder.layer.18.output.LayerNorm.weight :  False\n",
      "encoder.layer.18.output.LayerNorm.bias :  False\n",
      "encoder.layer.19.attention.self.query.weight :  False\n",
      "encoder.layer.19.attention.self.query.bias :  False\n",
      "encoder.layer.19.attention.self.key.weight :  False\n",
      "encoder.layer.19.attention.self.key.bias :  False\n",
      "encoder.layer.19.attention.self.value.weight :  False\n",
      "encoder.layer.19.attention.self.value.bias :  False\n",
      "encoder.layer.19.attention.output.dense.weight :  False\n",
      "encoder.layer.19.attention.output.dense.bias :  False\n",
      "encoder.layer.19.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.19.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.19.intermediate.dense.weight :  False\n",
      "encoder.layer.19.intermediate.dense.bias :  False\n",
      "encoder.layer.19.output.dense.weight :  False\n",
      "encoder.layer.19.output.dense.bias :  False\n",
      "encoder.layer.19.output.LayerNorm.weight :  False\n",
      "encoder.layer.19.output.LayerNorm.bias :  False\n",
      "encoder.layer.20.attention.self.query.weight :  False\n",
      "encoder.layer.20.attention.self.query.bias :  False\n",
      "encoder.layer.20.attention.self.key.weight :  False\n",
      "encoder.layer.20.attention.self.key.bias :  False\n",
      "encoder.layer.20.attention.self.value.weight :  False\n",
      "encoder.layer.20.attention.self.value.bias :  False\n",
      "encoder.layer.20.attention.output.dense.weight :  False\n",
      "encoder.layer.20.attention.output.dense.bias :  False\n",
      "encoder.layer.20.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.20.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.20.intermediate.dense.weight :  True\n",
      "encoder.layer.20.intermediate.dense.bias :  True\n",
      "encoder.layer.20.output.dense.weight :  True\n",
      "encoder.layer.20.output.dense.bias :  True\n",
      "encoder.layer.20.output.LayerNorm.weight :  False\n",
      "encoder.layer.20.output.LayerNorm.bias :  False\n",
      "encoder.layer.21.attention.self.query.weight :  False\n",
      "encoder.layer.21.attention.self.query.bias :  False\n",
      "encoder.layer.21.attention.self.key.weight :  False\n",
      "encoder.layer.21.attention.self.key.bias :  False\n",
      "encoder.layer.21.attention.self.value.weight :  False\n",
      "encoder.layer.21.attention.self.value.bias :  False\n",
      "encoder.layer.21.attention.output.dense.weight :  False\n",
      "encoder.layer.21.attention.output.dense.bias :  False\n",
      "encoder.layer.21.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.21.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.21.intermediate.dense.weight :  True\n",
      "encoder.layer.21.intermediate.dense.bias :  True\n",
      "encoder.layer.21.output.dense.weight :  True\n",
      "encoder.layer.21.output.dense.bias :  True\n",
      "encoder.layer.21.output.LayerNorm.weight :  False\n",
      "encoder.layer.21.output.LayerNorm.bias :  False\n",
      "encoder.layer.22.attention.self.query.weight :  False\n",
      "encoder.layer.22.attention.self.query.bias :  False\n",
      "encoder.layer.22.attention.self.key.weight :  False\n",
      "encoder.layer.22.attention.self.key.bias :  False\n",
      "encoder.layer.22.attention.self.value.weight :  False\n",
      "encoder.layer.22.attention.self.value.bias :  False\n",
      "encoder.layer.22.attention.output.dense.weight :  False\n",
      "encoder.layer.22.attention.output.dense.bias :  False\n",
      "encoder.layer.22.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.22.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.22.intermediate.dense.weight :  True\n",
      "encoder.layer.22.intermediate.dense.bias :  True\n",
      "encoder.layer.22.output.dense.weight :  True\n",
      "encoder.layer.22.output.dense.bias :  True\n",
      "encoder.layer.22.output.LayerNorm.weight :  False\n",
      "encoder.layer.22.output.LayerNorm.bias :  False\n",
      "encoder.layer.23.attention.self.query.weight :  False\n",
      "encoder.layer.23.attention.self.query.bias :  False\n",
      "encoder.layer.23.attention.self.key.weight :  False\n",
      "encoder.layer.23.attention.self.key.bias :  False\n",
      "encoder.layer.23.attention.self.value.weight :  False\n",
      "encoder.layer.23.attention.self.value.bias :  False\n",
      "encoder.layer.23.attention.output.dense.weight :  False\n",
      "encoder.layer.23.attention.output.dense.bias :  False\n",
      "encoder.layer.23.attention.output.LayerNorm.weight :  False\n",
      "encoder.layer.23.attention.output.LayerNorm.bias :  False\n",
      "encoder.layer.23.intermediate.dense.weight :  True\n",
      "encoder.layer.23.intermediate.dense.bias :  True\n",
      "encoder.layer.23.output.dense.weight :  True\n",
      "encoder.layer.23.output.dense.bias :  True\n",
      "encoder.layer.23.output.LayerNorm.weight :  False\n",
      "encoder.layer.23.output.LayerNorm.bias :  False\n",
      "pooler.dense.weight :  False\n",
      "pooler.dense.bias :  False\n"
     ]
    }
   ],
   "source": [
    "for name, param in auto_model.named_parameters():\n",
    "    print(name, \": \",param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments, losses\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "\n",
    "# https://www.sbert.net/docs/sentence_transformer/training_overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses.MultipleNegativesRankingLoss(gte_model)\n",
    "\n",
    "# https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss\n",
    "# https://arxiv.org/pdf/1705.00652 -- Section 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sbert.net/docs/package_reference/sentence_transformer/training_args.html#sentence_transformers.training_args.SentenceTransformerTrainingArguments\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"models/fine_tuned_gte\",\n",
    "\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=3, # default 3\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5, # default 5e-5\n",
    "    warmup_ratio=0.1, # Ratio of total training steps used for a linear warmup from 0 to learning_rate\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model = gte_model,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=loss,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887165c3a1714d71813910f0f6f6c32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9312, 'grad_norm': 2.674386739730835, 'learning_rate': 8.683853459972865e-06, 'epoch': 1.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d166bd1a8848bb9030a5d01822a96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 117.8969, 'train_samples_per_second': 111.148, 'train_steps_per_second': 6.947, 'train_loss': 2.793761014647245, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=819, training_loss=2.793761014647245, metrics={'train_runtime': 117.8969, 'train_samples_per_second': 111.148, 'train_steps_per_second': 6.947, 'train_loss': 2.793761014647245, 'epoch': 3.0})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./fine_tuned_gte_august22')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**. If you have run the codes above before, run the following instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = SentenceTransformer('./fine_tuned_gte_august22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8544/8544 [03:17<00:00, 43.23it/s]\n",
      "100%|██████████| 1101/1101 [00:24<00:00, 45.64it/s]\n",
      "100%|██████████| 2210/2210 [00:48<00:00, 45.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# fine-tuned vectorization:\n",
    "\n",
    "df_train['vector_ft'] = df_train['text'].progress_apply(lambda x: get_sentence_embedding(x, fine_tuned_model))\n",
    "df_dev['vector_ft'] = df_dev['text'].progress_apply(lambda x: get_sentence_embedding(x, fine_tuned_model))\n",
    "df_test['vector_ft'] = df_test['text'].progress_apply(lambda x: get_sentence_embedding(x, fine_tuned_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_parquet(\"SST-5_train_august27.parquet\")\n",
    "df_dev.to_parquet(\"SST-5_validation_august27.parquet\")\n",
    "df_test.to_parquet(\"SST-5_test_august27.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**. If you have run the codes above before, run the following instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(\"SST-5_train_august27.parquet\")\n",
    "df_dev = pd.read_parquet(\"SST-5_validation_august27.parquet\")\n",
    "df_test = pd.read_parquet(\"SST-5_test_august27.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
